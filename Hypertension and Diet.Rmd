---
title: "Factors that contribute to hypertension"
author: "Hanchao Zhang, M.S."
output:
  html_document:
    md_extensions: -startnum
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```




## Motivation


The link between hypertension and diet is well-established, with salt intake one of the most important risk factors for developing hypertension. Moving beyond the association of hypertension and salt intake, we focus here on other factors that might have an impact on having hypertension.

This case study will also introduce logistic regression and survey-weighted logistic regression, focusing on the difference between the two models. As we are using survey data, we show that survey-weighted logistic regression is a good choice of model in this setting.


We will analyze the association of hypertension and diet in our study. We primarily used ANOVA analysis and Post Hoc analysis using survey weighted data to get our conclusion that:





Reducing the quality of diet from excellent to good increase the odds of having hypertension by 1.06 ($P < 0.001$), reducing the quality of diet from excellent to fair increase the odds of having hypertension by 1.13 ($P < 0.001$), reducing the quality of diet from excellent to fair reduce the odds of having hypertension by 0.98 ($P < 0.001$) and reducing the quality of diet from average of very good and good to poor increase the odds of having hypertension by 0.97  ($P < 0.001$).

The reduction of diet quality to levels below the current diet quality increase the odds of having hypertension. The excpetion was made by changing from excellent to poor. However, using the average of the very good and good to poor to replace that result, the reduction of the diet quality, on the average, leads to increase in the odds of hypertension substantially.  











## What is the Data

1. [National Health and Nutrition Examination Survey (NHANES)](https://wwwn.cdc.gov/nchs/nhanes/default.aspx): A program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey started in the 1970???s and became a continuous program starting in 1999.
2. [R package nhanesA](https://cran.r-project.org/web/packages/nhanesA/vignettes/Introducing_nhanesA.html): Help you explore the NHANES data for surveys conducted since 1999.
3. [NYC HANES 2013-14 Blood Pressure Data](http://nychanes.org/data/): Provides various resourses such as 'Data','Variable List','Aanlytics Guideline'. All of them enable data users analyze the dataaset in the best way.
4. [Mortality Data](https://www.cdc.gov/nchs/data-linkage/mortality-public.htm): Contain information realted to survival outcomes of the survey participants. It would be useful if you want to further explore later survey.

## Data Preprocessing

### Load packages
Since this is the first R code chunk, we will load the necessary libraries.
```{r libraryLoad, warning=FALSE, message=FALSE}
library(knitr)
library(ggplot2)
library(tableone)
library(kableExtra)
library(survey)
library(broom)
library(GGally)
library(ggpubr)
library(Rmisc)
library(tidyverse)
library(sas7bdat)
```


### Load Data

*MAT: Where was this data downloaded from? Please add the URL or details on how you found it. Also, is there a data dictionary that defines the different variables? It would be useful to include it.*

NYC HANES 2013-14 Blood Pressure Data is downloded from http://nychanes.org/data/. It is a SAS formatted file so we will use the function `read.sas7bdat` from the `sas7bdat` library to read it into a data frame in R. 

```{r readData}
dat <- read.sas7bdat('./data/dat.sas7bdat')
```

This data set contains `r nrow(dat)` observations of `r ncol(dat)` different variables. For our analysis, we will want to select only a subset of the variables.

### Select the variables that we are interesed in

*MAT: can you include a link to the paper you are referring to or at least reference it?*

*MAT: I think you should add some details here of the variables you are considering: maybe a table with each variable name and a brief description of the different values?*

As we mentioned above, this is a survey dataset based on interview or questionaires with `r ncol(dat)` variables. Some vairables are meaningless to our research interest, such as 'LAQ1: What language is being used to conduct this interview'. For the covariates of interest, we checked the related paper and selected 11 covariates that have a potential association with hypertension according to the authors. 


For details on the meaning and encoding of the variables, see the accompanying file `./data/28283961_nyc_hanes_codebook_public_v2_102617.pdf`.


Here, we use the `select` function to choose and rename the columns that we want.
```{r selectCols}
hypertension_DF <- dat %>% select(
id = KEY,
age = SPAGE,
race = DMQ_14_1,
gender = GENDER,
born = US_BORN,
diet = DBQ_1,
income = INC20K,
diabetes = DX_DBTS,
bmi = BMI,
drink = ALQ_1_UNIT,
smoking = SMOKER3CAT,
surveyweight = WHQ_2LB,
hypertension = BPQ_2
)
```
Here are explanation for these variables:

-Non Categorical

  * id: Sample case ID
  * age: Sample age, range 22-115
  * surveyweight:How much does sample weigh in Pounds
  * bmi:BMI
  
-Categorical

  * race: 
    + 100 = White
    + 110 = Black/African American
    + 120 = Indian
    + 140 = Native Hawaiian/Other Pacific Islander
    + 180 = Asian
    + 250 = Other race
  * gender:
    + 1 = Male
    + 2 = Female
  * born: 
    + 1 = Us born
    + 2 = Other
  * diet: 
    + 1 = Excellent
    + 2 = Very good 
    + 3 = Good
    + 4 = Fair
    + 5 = Poor
  * diabetes: Previously diagnosed with diabetes
    + 1 = Diabetic dx
    + 2 = Diabetic but no dx
    + 3 = Not diabetic
  * drink: In the past 12 months, how often did SP drink any type of alcoholic beverage
    + 1 = Weekly
    + 2 = Monthly
    + 3 = Yearly
  * smoke: 
    + 1 = Never smoker
    + 2 = Current smoker
    + 3 = Former smoker
  * income:
    + 1 = Less than $20,000
    + 2 = $20,000 - $39,999
    + 3 = $40,000 - $59,999
    + 4 = $60,000 - $79,999
    + 5 = $80,000 - $99,999
    + 6 = $100,000 or more
  * hypertension: Previously diagnosed as hypertension
    + 1 = Yes
    + 2 = No
    
### Adjust the Data Type

The first step of any data analysis should be to explore the data through calculating various summary statistics. There are several ways that you can have a glance at your data. The `head` function is a nice way to get a quick look at the data, especially if there are not too many columns. Plotting the data or using the `summary` or `glimpse` functions are also excellent ways to understand your data at first sight.


The `summary` function tabulates categorical variables and provides summary statistics for continuous ones, while also including a count of missing values, which can be very important in deciding what variables to consider in downstream analysis.

```{r}
## find a charactor of the data for further adjustment
summary(hypertension_DF)
```
We find there are lots of NA's in our dataset and they just occupy a small part. So we aim to remove rows containing missing data with a nice function 'drop_na()' in library 'tidyr':

```{r}
hypertension_DF <- hypertension_DF %>%
  drop_na()
```

However, if NA's occupy large parts of the whole dataset, droping them directly is not a good choice. Finally we remain `r nrow(hypertension_DF)` observations of `r ncol(hypertension_DF)` different variables.

The `glimpse` function will return the name of the variables along with their type and some values. It helps you to check the levels and missing values in the categorical data. The `glimpse` function does something similar.

```{r firstLook}
## check the original data type
glimpse(hypertension_DF)
```

From the data summaries above, we can see that there are several categorical variables like race, gender, born, diet, income, diabetes, BMI, drink, and smoke, which are currently being treated as numerical values.

For these variables, it is important for us to adjust the coding, prior to doing any analysis in R. Since different value represent nothing but different categories. Sometimes people forget to change the type of data, and usually, it will give you a totally wrong result.

For example, in ` lm()` and ` glm()`, the function will treat numerically-coded categorical variables as continuous variables, which will give the wrong result. Instead, we want to convert these categorical variables to factors. Before doing this, we want to get a better understanding of exactly what values are stored in the different variables.


The first step for these categorical variables is to change them to factors. We refer to the codebook to get the correct mapping of the numerical values to the category labels.

*MAT: I ended up changing this section a fair amount. I don't think it makes sense to try to have some kind of automatic recoding of the numeric variables as factors when you still need to assign the correct levels in the end anyway. So I think doing it all at once, but one variable at a time, makes the most sense.*

We can use the `factor` function to convert each variable and assign the correct levels. Any values that are not included in the `levels` argument will get set to `NA` values.
```{r recodeFactors}
hypertension_DF$race <- factor(hypertension_DF$race, levels=c(100, 110, 120, 140, 180, 250), labels=c('White', 'Black/African American', 'Indian (American)/Alaska Native', 'Native Hawaiian/Other Pacific Islander', 'Asian', 'Some Other Race'))
hypertension_DF$gender = factor(hypertension_DF$gender, levels=c(1,2), labels=c('Male', 'Female'))
hypertension_DF$born = factor(hypertension_DF$born, levels=c(1,2), labels=c('US-born', 'Other'))
hypertension_DF$diet = factor(hypertension_DF$diet, levels=c(1:5), labels=c('Excellent', 'Very good', 'Good', 'Fair', 'Poor'))
hypertension_DF$income = factor(hypertension_DF$income, levels=c(1:6), labels=c('Less than $20,000','$20,000 - $39,999','$40,000 - $59,999','$60,000 - $79,999','$80,000 - $99,999','$100,000 or more'))
hypertension_DF$diabetes = factor(hypertension_DF$diabetes, levels=c(1:3), labels=c('Diabetic dx', 'Diabetic but no dx', 'Not diabetic'))
hypertension_DF$hypertension = factor(hypertension_DF$hypertension, levels=c(1,2), labels=c('Yes', 'No'))
hypertension_DF$drink = factor(hypertension_DF$drink, levels=c(1:3), labels=c('Weekly', 'Monthly', 'Yearly'))
hypertension_DF$smoking = factor(hypertension_DF$smoking, levels=c(1:3), labels=c('Never smoker', 'Current smoker', 'Former smoker'))
```

Check to see that things worked using the `summary` function:


### Plotting Categorical Data

Plotting numerical data is something you may be familiar with. This time we are going to incorporate some of the categorical variables into the plots. Although going from raw numerical data to categorical data bins does give you less precision, it can make drawing conclusions from plots much easier.

First, we try to plot one categorical variable 'hypertension', with one numerical variable 'age' and with one catergorical variable 'gender'. 

```{r, fig.height=7}
p1 <- ggplot(hypertension_DF, aes(x = hypertension, y=age)) + geom_boxplot()+ggtitle('Relationship between hypertension and age')
p2 <- ggplot(hypertension_DF, aes(x = hypertension, y=gender)) + geom_boxplot()+ggtitle('Relationship between hypertension and gender')
ggarrange(p1,p2)
```
It seems that the second plot fails to show the relationship! So we find it is not appropriate to apply traditional plotting methods on two categorical variables. Then, we think about the other way:

```{r}
ggplot(hypertension_DF, aes(x = hypertension, fill = gender)) + geom_bar()+ggtitle('Relationship between hypertension and gender')
```
This time it works! If you want to figure the proportion of male and female for each level, just change the position attribute of the bar plot to 'fill'.

```{r}
ggplot(hypertension_DF, aes(x = hypertension, fill = gender)) + geom_bar(position = "fill")+ggtitle('Relationship between hypertension and gender')
```
Interestingly, we find male are more likely to have hypertension than female. 

Simple data visualization will help you to make a first step judgement and provide much information. Plots indicate the trend, or pattern of the distribution of variables you interested in, and inspire you how to do the next step in data analysis. You can use this method to see the relationship between hypertension and other categorical variables.


### Check Normality of the Numeric Variables


After selecting the variables we want and recoding them, we now have a clean dataset. To explore the data more, we need to do some analysis on the data and to check some other features of them.

One thing that we want to do first is to check the normality of the continuous variables to help us decide what test statistics to use. For example, we use the t-test to check the difference of the normal distribution and Wilcoxon test to check the difference of non-normal distribution.

The numerical varialbes we have in our data set are `age` and `bmi`.

```{r}
par(mfrow = c(1,2))
for (i in c('age', 'bmi')) {
    qqnorm(hypertension_DF[,i], main = i)
    qqline(hypertension_DF[,i])
}
```


Based on these plots, both variables show some deviation from normality. Because of this, we should use the non-parametric(Wilcoxon) method to test for differences that involve these variables.


## Survey Weighted Data


So what model can we use in our case?

+ binary outcomes

You might think of logistic regression. Yes, you're pretty close already. However, think of the nature of our dataset. It's a data obtained from survey, and there is an important point to consider for analysis of survey data.




### What is Survey Weighted Data

It possible and indeed often happens to a perfectly designed sampling plan ends up with too many samples in a category. For example, too many women and not enough men, or too many white and not enough other races or both. Data weighting make sense for this kind of data. If we want the data to reflect the whole population, instead of counting the weight of each data point as one, we weight the data so that the sample we have can better reflect the entire community.



### What is the Weight of Data


Assuming that you have 25 students (20 male and 5 female) in your class, and you want to talk with 5 of them to know their understanding of the biostatistics class. By sampling 5 students from the total 25 students, you might get 5 all female students or 4 female and 1 male in your sample. Do you expect this sample to represent the population? Of course not. That's why we have to weight the observation so that we can make the sample better represent the population. The way of calculating the weight is:



$$Weight = \frac{Proportion~in~population}{Proportion~in~sample}$$
$$Male~Weight = \frac{20/25}{1/25} = 20$$
$$Male~Weight = \frac{5/25}{4/25} = 1.25$$


When we have multiple strata on the data, it might be troublesome to calculate the weight. However, for most of the survey data, the weight is calculated and included in the dataset. In our case study, the weight is calculated and we can simply apply the weight in the survey-weighted logistic regression.


### What is Finite Population Correction Factor

$$FPC = (\frac{N-n}{N-1})^{\frac{1}{2}}$$
+ N = population size
+ n = sample size


The finite population correction (fpc) is used to reduce the variance when a substantial fraction of the total population of interest has been sampled. It may not be appropriate if the target of inference is the process generating the data rather than the statistics of a particular finite population.


The Finite Population Correction Factor (FPC) is used when you sample without replacement from more than 5% of a finite population. It???s needed because under these circumstances, the Central Limit Theorem doesn???t hold and the standard error of the estimate (e.g. the mean or proportion) will be too big. In basic terms, the FPC captures the difference between sampling with replacement and sampling without replacement.


Most real-life surveys involve finite populations sampled without replacement. For example, you might perform a telephone survey of 10,000 people; once a person has been called, they won???t be called again. (https://www.statisticshowto.datasciencecentral.com/finite-population-correction-factor/)




```{r}
fpc <- ((5827719 - 1527)/(5827719-1))^0.5
fpc
```


### Fit the survey weighted logistic regression

There is a function ` svydesign()` in R package ` survey`. The function is used to specify a survey design. Some arguments that is useful in our analysis.

+ ids: Formula or data frame specifying cluster ids from largest level to smallest level, ~0 or ~1 is a formula for no clusters.

+ data: Data frame to look up variables in the formula arguments, or database table name

+ weights = Formula or vector specifying sampling weights as an alternative to ` prob`

+ fpc: Finite population correction


```{r}
hypertension <- hypertension_DF$hypertension
hypertension <- ifelse(hypertension == 'No',0,1)
hypertension_design <- svydesign(
  id = ~1,
  data = hypertension_DF[,-c(1,13)],
  weights = ~hypertension_DF$surveyweight,
  fpc = ~rep(fpc, nrow(hypertension_DF))
)


```




## Exploratory Data Analysis


### Survey Weighted Demographic table


The ` Createtableone` function from ` TableOne` package offers a survey-weighted version of demographic table. We can use the ` svyCreateTableOne` function output the demographic table for survey weighted data after the design was set up.






```{r, results='hide'}
factorvars<-c("race", "gender", "born", "diet", "income", "diabetes", "hypertension", "drink", "smoking")

svyCreateTableOne(data = hypertension_design, strata = 'hypertension', factorVars = factorvars) %>%
  print(nonnormal = c('age','BMI')) -> tblprint2
tblprint2
tblprint2[,4][c(2,3,10:12,18,25,29:31,35)] <- c('Wilcoxon', 'Chi-sq', 'Chi-sq', 'Chi-sq', 'Chi-sq', 'Chi-sq', 'Chi-sq', 'T-test', 'Chi-sq','Chi-sq', 'Chi-sq')
tblprint2[,1] == 'hypertension'
```


```{r}
kable(tblprint2 [-c(30,39),], "html", caption = "Stritified by hypertension", booktabs = T) %>%
  kable_styling('responsive') %>%
  add_indent(c(5:9,13:17,19:24,26:28,31:33,35:37))
```








### Eorrbarplots, boxplots and residual plots



Before doing those plots, we need to fit the model with the survey weighted data using ` svyglm()` function from the ` suvery` package.

It's similary to fit a normal logistic regression, the only difference is that instead of using the orignal data set in the ` data` argument, you should input the object from the ` svydesign()` to the ` design` argument in the function


```{r}
hypertension_design$variables$hypertension <- ifelse(hypertension_design$variables$hypertension == 'No', yes = 0, no = 1)

fit1 <- svyglm(hypertension ~ diet, design = hypertension_design)


```


After fitting the model, we can use our wrapper functions to do the data visualization and get some summary table that might be useful



As a data analyst, you probably will encounter this situation, writing similar codes for printing a table or visualization of your data. It can be quite annoying, and wrapper function can help you better solve you the trouble.

You can write a function that as general as possible so that it can solve as more problems as you wish.

There is something that you can do:

+ write your function and test your function for multiple data set

+ save your function as an R file

+ get your function from the directory you used to save the R file

And then, your function will appear, you can also save multiple functions in an R file, after quoting that R file, you will gain access for all the functions in that file



```{r}
source("svyplot.R")
```

```{r}
svyplot(fit1, output = 'table')

```

The table shows the number of people, the odds, and the standard deviance of having hypertension for each level.

Based on the table above, we can visualize it with errorbarplot and boxplot

```{r}
svyplot(fit1, output = 'errorbar')


```




## Data analysis



#### ANOVA and PostHoc Analysis


We can use the glht function from the multicomp package to do the PostHoc analysis. There're several ways to address the K matrix also known as comparision matrix. The easiest way is use Tukey to do the pairedwise comparision.




```{r}
multcomp::glht(fit1, linfct = multcomp::mcp(diet = "Tukey")) %>%
  summary() -> tbl_ph

tbl_ph
```


The ` turkey` returns us the pairedwise comparision. However, what if we have too many levels or we want to focus on some of the specific comparisons? 

The other way is to design our matrix of linear functions (` linfct`) in the function

First, we can take a look on the ` linfct` in ` turkey`


```{r}
tbl_ph$linfct
```


$$K \times Y = C   \times X$$

The linfct is the K matrix before the Y, the intercept represent the level of Excellent in our case, and if we want to compare any level with excellent, we can just input 1 at that level, if we want to compare other two levels except the reference level, we can set 1 and -1 at those two levels.

let's try the K matrix campring reference level with all other levels


```{r}
K <- rbind(
  "Excellent - Good" = c(0,0,1,0,0),
  "Excellent - Fair" = c(0,0,0,1,0),
  "Excellent - Poor" = c(0,1,0,0,1)
)
```


```{r}
multcomp::glht(fit1, linfct = multcomp::mcp(diet = K)) %>%
  summary() -> tbl1
tbl1
```


Even more, we can design a matrix that compare the average of some values


```{r}
K <- rbind(
  "Ave. of Very good and good - Poor" = c(0,0.5,0.5,0,-1)
  )

```


```{r}
multcomp::glht(fit1, linfct = multcomp::mcp(diet = K)) %>%
  summary() -> tbl2
tbl2
```




## Summary of the results


```{r}
tbl1 %>%
  tidy() -> tbl1

tbl2 %>%
  tidy() -> tbl2


tbl <- rbind(tbl1, tbl2)[c(1,3,4,5,6)]
tbl$estimate <- exp(tbl$estimate)
names(tbl)[1] <- 'Comparision'
tbl$p.value[tbl$p.value < 0.001] <- '< 0.001'
tbl %>%
  kable("html") %>%
  kable_styling('responsive') %>%
  add_indent(c(1:4))
```



Reducing the quality of diet from excellent to good increase the odds of having hypertension by 1.06 ($P < 0.001$), reducing the quality of diet from excellent to fair increase the odds of having hypertension by 1.13 ($P < 0.001$), reducing the quality of diet from excellent to fair reduce the odds of having hypertension by 0.98 ($P < 0.001$) and reducing the quality of diet from average of very good and good to poor increase the odds of having hypertension by 0.97  ($P < 0.001$).

The reduction of diet quality to levels below the current diet quality increase the odds of having hypertension. The excpetion was made by changing from excellent to poor. However, using the average of the very good and good to poor to replace that result, the reduction of the diet quality, on the average, leads to increase in the odds of hypertension substantially.  

















