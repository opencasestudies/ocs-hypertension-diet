---
title: OpenCaseStudies - Identifying factors that contribute to hypertension using NHANES data
author: Kexin Wang
output:
  html_document:
    keep_md: false
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE,warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, 
                      warning = FALSE)
options(digits = 2)
```

# Motivation

Hypertension is one of the most common diseases in the world. 
It has been associated with myocardial infarction, stroke, renal 
failure, and death if not detected early and treated appropriately.
Around 75 million American adults (32%) are estimated to have 
high blood pressure, costing the US around $48.6 billion 
each year. This total includes the cost of healthcare services, 
medications to treat high blood pressure, and missed days of work.

The link between hypertension and physical measurements has 
been well-established in previous studies. In this case study, we will explore the relationship between hypertension and a variety of risk factors.

This case study introduces survey methods for logistic regression, utilizing survey-weighted logistic regression and comparing the results to standard logistic regression. The plot below summarizes the results from this analysis 
illustrating that the standard error of coefficients calculated by the 
two models ([logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) 
and [survey-weighted logistic regression](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=9591E971AF4061BBF8F98083422FF313?doi=10.1.1.151.6423&rep=rep1&type=pdf))
are different. We will discuss later which model is a better choice 
for this dataset. 

<center>
![](data/FinalPlot.png)
</center>

The libraries used in this study are listed in the following table, 
along with their purpose in this particular case study:

|Library|Purpose|
|---|--------------------------------------------------------------------------------------------------------------|
|`ggplot2`| A system for declaratively creating graphics|
|`ggpubr`|Provides some easy-to-use functions for creating and customizing 'ggplot2'- based publication ready plots|
|`ggrepel`|Provides text and label geoms for 'ggplot2' that help to avoid overlapping text labels|
|`tidyverse`|A coherent system of packages for data manipulation, exploration and visualization |
|`kableExtra`|Helps with building common complex tables and manipulating table styles; creates awesome HTML tables|
|`survey`|Provides useful functions for analyzing complex survey samples|
|`haven`|A useful tool to import and export data from SAS, STATA, and SPSS formats|
|`plotrix`|A variety of plots, various labeling, axis and color scaling functions|
|`ggstance`|Provides flipped components for 'ggplot2': horizontal versions of 'Stats' and 'Geoms', and vertical versions of 'Positions'|
|`broom`|Takes the messy output of built-in functions in R, such as lm, nls, or t.test, and turns it into tidy data frames|


In order to run the code for this case study, please ensure you have these packages installed. 

The learning objectives for this case study include:

  * data visualization
  * logistic regression
  * survey-weighted analysis
  * selection of survey weights for unbalanced data

# What is the data?

For this case study, we will use the [New York City (NYC) Health and Nutrition Examination Survey (NYC HANES)](http://nychanes.org/), 
modeled on the 
[National Health and Nutrition Examination Survey (NHANES)](https://wwwn.cdc.gov/nchs/nhanes/default.aspx). NHANES is a population-based, cross-sectional study with data collected 
from a physical examination and laboratory tests, as well as a face-to-face 
interview and an audio computer-assisted self-interview (ACASI). It is 
designed to assess the health and nutritional status of adults and children 
in the United States. NYC HANES is a local version of NHANES, which implies 
it mainly focus on New York area. 

To access the NYC HANES data, go to the [NYC HANES data page](http://nychanes.org/data/) and  click the **NYC HANES Analytics Datasets** link; it will download automatically.  (A static link to this datafile can be found [here](http://nychanes.org/wp-content/uploads/sites/6/2019/01/public_v3_122018.sas7bdat).)

The data we will be using in this case study was collected from August 2013 to June 2014 and is called the 
**NYC HANES 2013-14 Blood Pressure Data**.  The survey used a probability sample of non-institutionalized adult 
New York City residents (ages 20 years or older) to provide 
representative citywide estimates. For further details, please refer to 
the [NYC HANES website](http://nychanes.org/).

In addition to the datafile, there are other useful resources available on the website including: 

* [Data Documents](http://nychanes.org/wp-content/uploads/sites/6/2019/01/28283961_Data-Documentation.pdf): 
Provides information on how best to analyze the NYC HANES 2013-14 dataset, given its population-based 
and clustered sampling scheme.
* [Analytics Guideline](http://nychanes.org/wp-content/uploads/sites/6/2015/11/ANALYTIC-GUIDELINES-2016_V2.pdf): 
Provides overall guidance on the use of the NYC HANES 2013-14 dataset and statistical weights, 
as well as other analytic issues pertaining to assessing statistical reliability of estimates.
* [Variable Codebook](http://nychanes.org/wp-content/uploads/sites/6/2019/01/28283961_NYC-HANES_codebook_Public_V3_011019.pdf): Defines the variables included in the dataset and describes how values for these variables are coded.
* [Weight Adjustment](http://nychanes.org/wp-content/uploads/sites/6/2015/11/NYC-HANES-Training-Slides_part-2_08222016.pdf): 
Explains how NYC HANES data are weighted in order to compensate for unequal probability of selection and explains how to choose the correct weight for analysis.
* [Questionnaire](http://nychanes.org/wp-content/uploads/sites/6/2015/11/28283961_NYC-HANES-2013-14_Questionnaire.pdf): Shows the questionnaire that participants in the study completed.

All of these documents enable data analysts to understand the definitions and coding of the variables and then complete the analysis appropriately.

# Data import

First we load the libraries needed for the case study.

```{r library-load}
library(knitr)
library(ggplot2)
library(ggpubr)
library(ggrepel)
library(tidyverse)
library(kableExtra)
library(survey)
library(haven)
library(broom)
library(plotrix)
```

The NYC HANES data file we are working with is a SAS formatted file, so 
we will use the function `read_sas()` from the `haven` library 
to read in the data and create a [tibble](https://tibble.tidyverse.org) 
(or `tbl_df`) in `R`. Tibbles are nice because they do not change variable 
names or data types, and they have an enhanced `print()` method which
makes it easier to view the data when working with large datasets containing complex objects. 
The [`haven` library](https://www.rdocumentation.org/packages/haven/versions/2.1.0) 
is useful to import and export files saved in a variety of formats such as
[SAS, STATA, and SPSS](http://stanfordphd.com/Statistical_Software.html). 

We now read in the data and check the dimensions of the data object:
```{r read-data}
dat <- read_sas('./data/d.sas7bdat')
dim(dat)
```

Our data contains `r nrow(dat)` observations on `r ncol(dat)` 
different variables. For our analysis, we will only consider a subset of these variables.


# Data wrangling

## Select the variables (or columns)

This is a survey dataset based on interviews and questionnaires
with `r ncol(dat)` variables. Some variables are not relevant 
to our current research question, such as 
_'LAQ1: What language is being used to conduct this interview'_. 

Previous work has shown hypertension is associated  with 
drinking, smoking, cholesterol values, and triglyceride levels. 
Instead of looking at these same risk factors, we will consider whether other variables -- which 
at first might not seem highly related to hypertension -- have an association with 
hypertension after all. We selected 13 covariates for our 
analysis.  

We will use the `select()` function from the `dplyr` package
to choose and rename the columns that we want. 

Here is a simple example to show how the renaming of the column names works:

```{r}
rename <- 
  dat %>% 
    select(id = KEY,
           race = DMQ_14_1,
           diabetes = DX_DBTS)
colnames(rename)
```

In this example, we select the three variables of `KEY`, `DMQ_14_1`, and `DX_DBTS` and rename them to the more descriptive `id`, `race`, and `diabetes`.  We save this smaller and renamed data frame in the `rename` object rather than write over our originial data. Undoubtedly, compared with `DMQ_14_1` and `DX_DBTS`, 
`race` and `diabetes` are more readable and more easily to understand.

Now we select and rename the 13 variables we will consider in our analysis:

```{r select-cols}
hypertension_df <- 
  dat %>% 
    select(id = KEY,
           age = SPAGE,
           race = DMQ_14_1,
           gender = GENDER,
           diet = DBQ_1,
           income = INC20K,
           diabetes = DX_DBTS,
           bmi = BMI,
           cholesterol = BPQ_16,
           drink = ALQ_1_UNIT,
           smoking = SMOKER3CAT,
           hypertension = BPQ_2,
           surveyweight = CAPI_WT)
```

We will give some description of each variable below, but for full details we refer the reader to the
[Variable Codebook](http://nychanes.org/wp-content/uploads/sites/6/2019/01/28283961_NYC-HANES_codebook_Public_V3_011019.pdf).


### Non-categorical variables 

There are four non-categorial variables that we will use in our analysis:

  * `id`: Sample case ID, unique to each individual in the sample
  * `age`: Sample age, range 22-115 years
  * `bmi`: BMI = $kg/m^2$ where $kg$ is a person's weight in kilograms and $m$ is their height in meters
  * `surveyweight`: Numeric values associated with each observation to let us know how much weight the observation should receive in our analysis (more details later)
  
### Categorical variables 

We will consider ten categorial variables: 

  * `race`: 
    + 100 = White
    + 110 = Black/African American
    + 120 = Indian
    + 140 = Native Hawaiian/Other Pacific Islander
    + 180 = Asian
    + 250 = Other race
  * `gender`:
    + 1 = Male
    + 2 = Female
  * `born`:
    + 1 = US born
    + 2 = Other country
  * `diet`: 
    + 1 = Excellent
    + 2 = Very good 
    + 3 = Good
    + 4 = Fair
    + 5 = Poor
  * `diabetes`: Previously diagnosed with diabetes
    + 1 = Diabetic with diagnosis
    + 2 = Diabetic without diagnosis 
    + 3 = Not diabetic
  * `cholesterol`: an oil-based substance. If concentrations get too high, it puts people at risk of heart diseases
    + 1 = High cholesterol value
    + 2 = Low cholesterol value
  * `drink`: In the past 12 months, how often did sample drink any type of alcoholic beverage
    + 1 = Weekly
    + 2 = Monthly
    + 3 = Yearly
  * `smoke`: 
    + 1 = Never smoker
    + 2 = Current smoker
    + 3 = Former smoker
  * `income`:
    + 1 = Less than $20,000
    + 2 = $20,000 - $39,999
    + 3 = $40,000 - $59,999
    + 4 = $60,000 - $79,999
    + 5 = $80,000 - $99,999
    + 6 = $100,000 or more
  * `hypertension`: Previously diagnosed as having hypertension
    + 1 = Yes
    + 2 = No
    
## Initial data inspection

The first step of any data analysis should be to explore 
the data through data visualizations and data summaries like
tables and summary statistics. There are several ways that you 
can have an initial glance at your data. The `summary()` or `head()` functions in are excellent 
ways to help you have a quick look at the data set.

The `summary()` function tabulates categorical variables and 
provides summary statistics for continuous variables, while also 
including a count of missing values, which can be very important 
in deciding what variables to consider in downstream analysis.

```{r}
summary(hypertension_df)
```

We see that certain variables have a large number of `NA` values; in 
particular `drink` has `r sum(is.na(hypertension_df$drink))` 
`NA` values and `diabetes` has `r sum(is.na(hypertension_df$diabetes))` 
`NA` values. Directly removing rows containing missing data is not desirable 
considering the large number of such rows, so we decided to look more 
closely at the missing values. Using the 
[Variable Codebook](http://nychanes.org/wp-content/uploads/sites/6/2019/01/28283961_NYC-HANES_codebook_Public_V3_011019.pdf) again, 
we found another variable `AlQ_1` (how often did the survey participant drink any type of alcoholic beverage), 
where 0 means they never drink.

Let's look at the frequency of counts of the variable with 
function `table()`.

```{r}
table(dat$ALQ_1)
```

Now we see why there are so many missing values for `drink`. 
Among these `r sum(is.na(hypertension_df$drink))` missing values, 
`r sum(dat$ALQ_1 == 0, na.rm = TRUE)` samples never drink and 
there are just 
`r sum(is.na(hypertension_df$drink)) - sum(dat$ALQ_1 == 0, na.rm = TRUE)`
actual missing values. Therefore, merging these two variables 
as one is a better way to capture drinking that includes those who never drink. We label those subjects who answered `0` to variable `AlQ_1` (those who never drink) as 4.
```{r}
hypertension_df$drink[which(dat$ALQ_1==0)] <- 4
```

Now our variable `drink` has values as follows:

  * `drink`: In the past 12 months, how often did sample drink any type of alcoholic beverage
    + 1 = Weekly
    + 2 = Monthly
    + 3 = Yearly
    + 4 = Never

And we can see only 6 missing values for `drink` are left now.
```{r}
summary(hypertension_df$drink)
```

In the next step, we aim to remove rows containing missing 
data with the function `drop_na()` in library `tidyr` and store in a new data frame:

```{r}
hy_df <- 
  hypertension_df %>%
  drop_na()
```

This will drop all rows which still contain missing values. By checking the dimensions of our new data frame, `hy_df`, we see that we retain `r nrow(hy_df)` observations with `r ncol(hy_df)` different variables.
```{r}
dim(hy_df)
```

## Adjust data types

From the data summaries above, we can see that there are several 
categorical variables like `race`, `gender`, `born`, `diet`, `income`, 
`diabetes`, `bmi`, `drink`, and `smoke`, which are currently being 
treated as numerical values, but should be treated as factors. Treating the data as numerical instead of categorial could result in problems in the downstream analysis.

For example, the linear model `lm()` and generalized linear model `glm()` 
functions will treat numerically-coded categorical variables as 
continuous variables, which will give the wrong result. Instead, 
we want to convert these categorical variables to factors. Before doing 
this, we want to get a better understanding of exactly what values 
are stored in the different variables. We refer to the 
[Variable Codebook](http://nychanes.org/wp-content/uploads/sites/6/2019/01/28283961_NYC-HANES_codebook_Public_V3_011019.pdf) 
to get the correct mapping of the numerical values 
to the category labels.

We can use the `factor()` function in base R to convert each 
variable and assign the correct levels. Any values that are not 
included in the `levels` argument will get set to `NA` values. 
We also want to think about creating a natural ordering to the 
factor levels here: the first level will generally be our 
reference level in a linear model, so it makes sense to try
to give them an order that reflects our choice of reference group.  For example, we will want to compare individuals drink weekly, monthly, or yearly to those who never drink, so we order the `drink` variable as `Never`, `Weekly`, `Monthly`, `Yearly` even though the numerical values in that order are 4, 1, 2, 3.

```{r recodeFactors}
hy_df$race <- factor(hy_df$race, 
                     levels=c(100, 110, 120, 140, 180, 250), 
                     labels=c('White', 'Black/African American', 
                              'Indian /Alaska Native', 
                              'Pacific Islander', 
                              'Asian', 'Other Race'))

hy_df$gender <-  factor(hy_df$gender, levels=c(1,2), 
                        labels=c('Male', 'Female'))

hy_df$diet <-  factor(hy_df$diet, levels=c(5:1), 
                      labels=c('Poor', 'Fair', 'Good', 
                               'Very good','Excellent'))

hy_df$income <-  factor(hy_df$income, levels=c(1:6), 
                        labels=c('Less than $20,000','$20,000 - $39,999',
                                 '$40,000 - $59,999','$60,000 - $79,999',
                                 '$80,000 - $99,999','$100,000 or more'))

hy_df$diabetes <-  factor(hy_df$diabetes, levels=c(3,1,2), 
                          labels=c('Not diabetic','Diabetic dx','Diabetic but no dx'))

hy_df$cholesterol <-  factor(hy_df$cholesterol, levels=c(2,1), 
                             labels=c('Low value','High value'))

hy_df$drink <-  factor(hy_df$drink, levels=c(4,1,2,3), 
                       labels=c('Never','Weekly', 'Monthly', 'Yearly'))

hy_df$smoking <-  factor(hy_df$smoking, levels=c(3:1), 
                         labels=c('Never smoker','Former smoker','Current smoker'))

hy_df$hypertension <-  factor(hy_df$hypertension, levels=c(2,1), 
                              labels=c('No','Yes'))
```

Now let's look at a summary of our `hy_df` data frame. 

```{r}
summary(hy_df)
```

We see that now the `summary()` function gives us counts for each category of these variables rather than the meaningless numerical summaries, like means, it was giving before.

# Exploratory data analysis

Simple data visualizations give us a first look at 
the data and provide information about how the different 
variables are related to one another. Plots can identify the trends 
or patterns in the variables of interest and inform the next steps in the data analysis. For our 
data visualizations, we will mainly use the package `ggplot2`, 
a powerful tool for data visualization. A link for its cheat sheet is [here]( https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf).

The type of plot we will make will depend on the type of variable(s) we are plotting.  First, we try to plot one categorical variable (`hypertension`) with 
one numerical variable (`age`). Since `hypertension` is the categorical variable, we can compare the age distributions between those who are hypertense and those who are not using side-by-side boxplots.  Here the variable on the `x` axis is `hypertension` and the variable on the `y` axis is `age`, which corresponds to the `aes(x=hypertension, y=age)` definition in the `ggplot` aesthetic definition.

```{r, fig.width=4, fig.height=4}
p1 <- hy_df %>% 
        ggplot(aes(x = hypertension, y = age)) +
          geom_boxplot() + 
          ggtitle('Distribution of age by hypertension status')
p1
```

Comparing the medians of these two boxplots, we see that people who are hypertense tend to be older than people who are not, which indicates that age is related to hypertension.

What about a plot for investigating the relationship between `hypertension` and `gender`? Let's try three different ways to plot the categorical variable `gender` with `hypertension`.  We'll use the function `ggarrange()` in [package `ggpubr`](https://www.rdocumentation.org/packages/ggpubr/versions/0.2) to arrange multiple ggplots on the same page. 

```{r, fig.width=11, fig.height=5}
p2 <- hy_df %>% 
        ggplot(aes(x = hypertension, y=gender)) + 
          geom_boxplot() + ggtitle('distribution of gender')
p3 <- hy_df %>% 
        ggplot(aes(x = hypertension, fill = gender)) + 
          geom_bar() + ggtitle('distribution of gender')
p4 <- hy_df %>% 
        ggplot(aes(x = hypertension, fill = gender)) + 
          geom_bar(position = "fill") + 
          ggtitle('distribution of gender') + 
          ylab('proportion')
ggarrange(p2, p3, p4, ncol=3, nrow=1)
```

The left plot uses `geom_boxplot()` as we did with `age`, but 
it fails to show the relationship of interest! Boxplots are not what we want 
for a categorical variable like `gender`, so we need another plotting method. 

The middle and right plots use barplots to look at the 
relationship between `hypertension` and `gender`. This time it works! The middle plot shows the count of males and females for those with and without hypertension.  The right plot more clearly shows the proportion of males and females within each hypertension group by using the `position='fill'` option in the `geom_bar()` function. The y-axis in the right plot is proportion rather than count. 
From this visualization, we see that a higher proportion the hypertense individuals are male compared to the non-hypertense individuals.

But are these plots really the right ones for our question of interest?  Rather than comparing the distribution of gender between the hypertension groups, we really want to see whether there is a difference in hypertension rates between gender groups. To do this, we switch the x-axis and y-axis, to show `hypertension` as a 
function of `gender`, rather than the other way around. 

```{r, fig.width=11, fig.height=5}
p3b <- hy_df %>% 
        ggplot(aes(x = gender, fill = hypertension)) + 
          geom_bar() + ggtitle('distribution of hypertension')
p4b <- hy_df %>% 
        ggplot(aes(x = gender, fill = hypertension)) + 
          geom_bar(position = "fill") + 
          ggtitle('distribution of hypertension') + 
          ylab('proportion')
ggarrange(p3b, p4b, ncol=2, nrow=1)
```

Now we can compare the distribution of hypertension between males and females.  
From this visualization we see that a higher proportion of males than females in our dataset have hypertension. 

Next, we apply the same visualization methods on other variables to explore relationships between them.

```{r, fig.width=9, fig.height=5}
p5 <- hy_df %>% 
        ggplot(aes(x = smoking, fill = drink)) + 
          geom_bar(position = "fill") + 
          ggtitle('smoking and drink') + ylab('proportion')
p6 <- hy_df %>% 
        ggplot(aes(x = cholesterol, fill = diabetes)) + 
          geom_bar(position = "fill") + 
          ggtitle('cholesterol and diabetes') + ylab('proportion')
ggarrange(p5, p6, ncol=2, nrow=1)
```

From the left plot, we see that there are higher proportions of monthly and yearly drinkers in current smokers, and a higher proportion of weekly drinkers in former smokers.  This may imply a potential relationship between smoking and drinking. 
Additionally, the right plot shows that there is a much higher proportion of
diabetes, both diagnosed and undiagnosed, in the high cholesterol group compared to the low cholesterol group.  Therefore a relationship also appears to exist between these two variables. 

What if we want to explore a relationship between more than two variables? 
Suppose we wanted to look at the relationship between BMI, cholesterol level, and smoking status all at the same time.  Since `cholesterol` and `smoking` are both categorical variables, we can divide by the levels of one of these variables, say `smoking`, and then look at plots showing the relationship between the other two variables within each level of `smoking`.

We can do this in `ggplot` by taking the plot statement for two variables (`cholesterol` and `bmi`) and adding a `facet_wrap()` function for the third variable we're dividing by.

```{r}
p7 <- hy_df %>% 
        ggplot(aes(x = cholesterol, y = bmi, fill = cholesterol)) +
          geom_boxplot() + facet_wrap(~ smoking, ncol = 3)
p7
```

The function `facet_wrap(~ smoking, ncol = 3)` uses the 
faceting capability to produce a plot with three panels 
(one panel for each smoking category). Within each panel 
is a boxplot which characterises the distribution of `bmi` for 
each cholesterol category within that `smoking` category. We 
realize that there is a tendency of increasing `bmi` from never 
smoker to current smoker, and the highest `bmi` values are 
found in current smokers.

These data visualizations have revealed potential relationships between our variables.  The next step is to choose which variables
we might include our statistical model in the next section.  

# Data analysis

Now that we have spent some time cleaning the data and looking at 
data visualizations, we want to use a statistical 
model to address our question of interest about which 
factors are related to the risk of hypertension.

Which model should we use? Since we are looking 
at whether or not someone develops hypertension, our outcome
variable (`hypertension`) is **binary**. A binary outcome means a logistic regression model is a natural choice.  However, think of the nature of our dataset and how it was collected. It is data obtained from a survey, and we have to account for this during the analysis of the data.

In a survey sample, we often end up with "too many" samples in a category, often due to the designed sampling plan.  By "too many", we mean more than would be expected based on the make-up of the population from which we are sampling.  For example, we may have a much higher proportion of women in our sample compared to the population and a much lower proportion of men than in the population. This may happen by design if we purposely *oversample* a group that isn't well represented in the overall population.

To analyze our survey data and infer back to the population, we can use data weighting to account for the mismatch between the population and sample. If we want the data to reflect the whole population, instead of treating each data 
point equally, we weight the data so that taken together,
our sample does reflect the entire community.

## Survey weights 

### What are survey weights?

Suppose that we have 25 students (20 male and 5 female) in your biostatistics
class, and we want to talk with 5 of them to gauge their 
understanding of the content in the class. Although the proportion of female students in the population is small, we are very interested in getting their opinion, so we want to be sure to have some female students in our sample.  By randomly sampling 5 students 
from the class, it's quite possible we could end up with all male students in our sample, and we wouldn't learn anything about the female perspective in the class. 

Consider the extreme case where we are going to require that 4 of the 5 people we sample are female students, to be sure we get good information about the female perspective.  We sample 4 of the 5 female students and 1 of the 20 male students.   Do we expect this sample to represent the population? Definitely not, since there is a higher proportion of females in the sample than the population. We can 
correct for this by weighting our samples so that, taken together, 
they better reflect the composition of the population we want to learn about. 

Let's assume we sampled 4 of the 5 female students and 1 of the 20 male students from our population. To calculate the survey weights, we could use the following formula:

$$Weight = \frac{Proportion~in~population}{Proportion~in~sample}$$
$$w_m=Male~Weight = \frac{20/25}{1/25} = 20$$
$$w_f=Female~Weight = \frac{5/25}{4/25} = 1.25$$
We can interpret these weights by saying that each male student in the sample represents 20 male students in the population and each female student in the sample represents 1.25 female students in the population.  Mathematically, we can see this as:

$$ 1~observed~male* w_m = 20~males $$ 
and 
$$ 4~observed~females * w_f = 5~females$$ 

<center>
![](data/surveyweight.jpeg)
</center>

By weighting the observations, we make the sample better 
represent the population.

For complex survey sampling designs, it can be 
complicated to calculate the weight for each individual observation. However, for many large survey data sets, such as NHANES, the appropriate weight is calculated by the organization that administers the survey and provided as a variable in the dataset. In our case study, this survey weight is calculated and provided as the `surveyweight` variable and we can simply 
apply this weight and perform a **survey-weighted logistic regression**.

### Selecting the weights

Because the NYC HANES 2013-2014 data have been collected to 
address a variety of different questions and using different 
surveys, the researchers who produced the data have employed a 
somewhat complex weighting scheme to compensate for unequal 
probability of selection. Five sets of survey weights have been 
constructed to correspond to different sets of variables that were
collected: CAPI  weight, Physical weight, Blood Lab result weight,
Urine Lab results weight and Salica Lab results weight. 
**The determination of the most appropriate weight to use for a specific analysis depends upon the variables selected by the data analyst**. 

When an analysis involves variables from different components
of the survey, the analyst should decide whether the outcome 
is inclusive or exclusive, and then choose certain weights. 
To learn how to use weights for different purposes, refer to the particular
[Analytics Guidelines](http://nychanes.org/wp-content/uploads/sites/6/2015/11/ANALYTIC-GUIDELINES-2016_V2.pdf) for the survey. 

In our case, we choose CAPI weight, which should be used to analyze 
participants responses to all interview questions. All of our 
survey participants have a value for the `CAPI_WT` variable. 
We define hypertension as the prior diagnosis of high 
blood pressure, so we should use the most inclusive one, 
CAPI weight, to get an inclusive outcome.  We've select this variable and renamed it as `surveyweight` in the earlier data cleaning part of this analysis. 

## Finite population correction factor

Now that we have discussed how to weight our samples, 
there is one more technical detail that we need to 
address when using survey data. Many methods for analysis 
of survey data make the 
**assumption that samples were collected using sampling with replacement**, 
i.e., any time a sample is drawn, each person in the population has 
an equal chance of being sampled, even if they have 
already been sampled. This is not usually how surveys
are actually carried out, so an adjustment may be necessary
to reflect this difference. This adjustment is called the
**finite population correction factor** and it is defined as:

$$FPC = (\frac{N-n}{N-1})^{\frac{1}{2}}$$
 
* `N` = population size
* `n` = sample size

In the case when the assumption above is violated (e.g. if you 
are sampling a sufficiently large proportion of the population), 
then you might sample the same persion twice. The finite 
population correction (FPC) is used to reduce the variance when
a substantial fraction of the total population of interest has 
been sampled. We got the value of `N` and `n` on the
[Analytics Guideline](http://nychanes.org/wp-content/uploads/sites/6/2015/11/ANALYTIC-GUIDELINES-2016_V2.pdf). 
Next let's calculate the FPC as below:

```{r}
N <-  6825749
n <- 1065
((N-n)/(N-1))^0.5
```

The FPC of our data set is very close to 1 and, in general, 
you can ingore it. But technically, since the data were
collected through sampling without replacement, 
it is more appropriate to use it.

## Specify the survey design

We now need to figure out how to specify the survey design 
and incorporate our sampling weights in our modeling steps. 
To help us do this, we use thefunction `svydesign()` in 
[package `survey`](https://cran.r-project.org/web/packages/survey/survey.pdf).
This function combines a data frame and all the design information needed to specify 
a survey design. Here is the list of options provided 
in this function:

* `ids`: Specify it for cluster sampling, `~0` or `~1` is a formula for no clusters. Cluster sampling is a multi-stage sampling, the total population are divided into several clusters and a simple random sample of clusters are selected. Each element in these clusters are then sampled.

* `data`: Data frame to look up variables in the formula arguments, or database table name

* `weights`: Formula or vector specifying sampling weights as an alternative to `prob`

* `fpc`: Finite population correction, `~rep(N,n)`  generates a vector of length n where each entry is N (the population size). Default value is 1. The use of fpc derives a without replacement sample, otherwise the default is a with replacement sample.
 
* `strata`: Specify it for stratified sampling, which divides members of the population into homogeneous subgroups and then sample independently in these subpopulations. It is advantageous when subpopulations within an overall population vary.
 
 
Now we use some options to create the design relative to our dataset:

```{r}
hypertension_design <- svydesign(
  id = ~1,
  #fpc = ~rep(N,n),
   weights = ~hy_df$surveyweight,
  data = hy_df[,-c(1,13)]
)
```
The arguments are interpreted as the following:

* `ids = ~1` means there is no cluster sampling
* `data = hy_df[,-c(1,13)]` tells `svydesign` where to find the actual data
* `weights= ~hy_df$surveyweight` tells it where to find the weight in our data frame

`summary()` shows the results:
```{r}
summary(hypertension_design)
```

"Independent sampling design" means our sampling design is a 
simple random sample (SRS). By setting other parameters 
you can also specify different kinds of designs, such as 
stratified or cluster sampling, etc.


## Calculate survey-weighted summary statistics

Once we have created our `survey.design` object, we can use the
convenient `svy*` functions to calculate summary statistics that
account for survey design features.

To calculate the mean and its standard error, use the function 
`svymean()`. The `svymean()` functino performs a weighted estimation
with each observation being weighed rather than direct calculation. 
We can compare this result to ignoring the survey weights using the
`mean()` and `std.error()` functions in base R:

```{r}
svymean(~bmi, hypertension_design)
```
```{r}
mean(hy_df$bmi)
std.error(hy_df$bmi)
```

It seems that there is not a very large difference between 
these two values and their standard errors. Since this is a
simple random sample the difference is not that obvious. 
But it will give precision estimates that incorporate the 
effects of stratification and clustering if you choose more 
complex sample design. 

To calculate the confidence interval, we can use the 
function `confint()` directly:

```{r}
confint(svymean(~bmi, hypertension_design))
```

Subgroup statistics are also easy to calculate with
function `svyby()`:
```{r}
svyby(~bmi, by=~diet, design=hypertension_design, 
      FUN = svymean)
```

If you are particularly interested in one group, 
you can use function `subset()` :


For example, if we are only interested in females
```{r}
svymean(~bmi, subset(hypertension_design,gender=="Female"))
```

or males 
```{r}
svymean(~bmi, subset(hypertension_design,gender=="Male"))
```

It seems that mean bmi and the associated SEs are a 
little higher for females compared to males. 


## Survey-weighted logistic regression

### Fit a simple model

$logit(p)= log(\frac{p}{1-p)})=X^{T}\beta$, where $p=P(Y=1)=E(Y)$ 

Logistic regression is widely used to deal with binary response 
variable. As we mentioned above, our dataset is a survey dataset, 
and we need to take survey weights into account, so therefore we 
need to fit the model with the survey weighted data. To do this,
we can just use the `svyglm()` function from the `survey` package.

It's similar to fitting a normal logistic regression, and the 
only difference is that instead of using the original data set 
in the `data` argument, you should input the object from the 
output of `svydesign()` to the `design` argument in the function.

First, we need to recode our factor output as a binary (0-1) variable. 

```{r}
hypertension_design$variables$hypertension <- 
  ifelse(hypertension_design$variables$hypertension == 'No', 
         yes = 0, no =1)
```

Then we fit our survey-weighted logistic regression model and 
look at the summarized output. Firstly, just start with a simple
model by choosing one variable. `smoking` is chosen for test:

```{r}
g <- svyglm(hypertension ~ smoking, 
    family = quasibinomial(), design = hypertension_design)
```

`tidy()` function in `broom` package can provide a dataframe 
representation about the models output. Just put the model
name in it and you will receive a nice dataframe!

```{r}
g_res <- tidy(g)
g_res
```

All of the prediction variables are list in `term` column and the left part 
is its corresponding estimation value, standard error etc. After substracting 
coefficient value and standard error, you can build confidence interval by hand.

What does the model output tells us about the relationship 
between predictor variables and the chance of getting hypertension? 
Look at coefficient table. For predictor variable `smoking`,
baseline setting is 'Never smoker'. Exponentiated coefficients
for 'Former smoker' is `r exp(g_res$estimate)[2]`, 
which means 'Former smoker' is associated with an odds of getting hypertension of 56% 
of that for 'Never smoker'; Similarly, 'Current smoker' is 
associated with an odds of getting hypertension of 69% 
of that for 'Never smoker'. What's more, we can see the 
p-value for `smoking` are all less than 0.05, suggesting 
their significance in this case.


### Fit a full model

After being familiar with how to interpret output of 
logistic regression model, try to fit a full model 
including all variables: 

```{r}
g1 <- svyglm(hypertension ~ 
               age + race + gender + diet + income + 
               diabetes + bmi + cholesterol + drink + smoking,
             family = quasibinomial(), 
             design = hypertension_design)
g1_res <- tidy(g1)
#g1_res[-3]
```

One interesting thing is that both the exponentiated 
coefficients and the p-value for `smoking` in this model 
increase sharply! Why will we get this change? 

Do you remember in section Exploratory data analysis, we found
there was a correlation between `smoking` and other variables.
This finding proves that when adding other variables, 
influence of former predictor variables are likely to change.

Now, let's interpret the new output. One categorical
variable and one continuous variable are chosen as examples:

* `smoking`: Holding all other variables the same, 'Former smoker' 
is associated with an odd of getting hypertension 
of `r exp(g1_res$estimate)[25]` of that for 'Never smoker'; 
'Current smoker' is associated with an odd of getting hypertension 
of `r exp(g1_res$estimate)[26]` of that for 'Never smoker'. 

* `bmi`: Holding all other variables the same, a unit increase in bmi 
changes odds by `r exp(g1_res$estimate)[20]`.

This step is pretty meaningful since it allows us to achieve 
more accurate result. When we try to understand the output,
holding all other variables the same is crucial. If we just 
do analysis based on the simple model, we cannot guarantee 
all other variables keep constant. However, the advantage of 
full model is that we compare all variables with others stable,
maximizing the chance to perform the same level comparison. 
Therefore our interpretation could be more precise and rational.


### Model selection

Not all variables in our model `g1` are significant, so we would 
like to remove some of them to get a reduced model. We find 
`race`,`income`, `diet`,`drink` and `smoking` are not that 
sigificant. Contrary to our common sense, `diet`,`drink` and 
`smoking` have minimum influence on hypertension when including
the other variables in the model. We believe this is because 
that their influence may be explained by other covariates, 
such as `bmi`. Therefore, here we just remove `race`, `drink` and `income`.

```{r,fig.width=7, fig.height=5}
g2 <- svyglm(hypertension ~ 
        age + bmi + cholesterol + diabetes + gender +
        smoking + diet, 
        family = quasibinomial(), 
        design = hypertension_design)
```

Using this model output, we would like to further examine 
which covariates have the larger influence on hypertension. 
To do this, we compare their effect sizes in the plot below.

```{r}
s <- data.frame('c' = exp(g2$coefficients[2:12]),
                'n' = names(g2$coefficients)[2:12])
s <- s[order(s$c),]
l = seq(from = 2, to = 12, by = 1)

plt<- ggplot(s, aes(x = l, y = c,col = s$c)) + 
  geom_point(size = 5)  + 
  scale_colour_gradient(low ='pink', high = 'darkred')+
  geom_hline(yintercept = 1,col = 'pink',size = 1.5, linetype = 2)+
  theme_bw() + theme(plot.title = element_text(hjust = 2, vjust = -2))+
  geom_text_repel(aes(y = c,label = n), col = 'darkred')+
  ggtitle("Effect on Hypertension") + 
  ylab("Odds Ratio") + 
  xlab("Features")+
  coord_flip()
  
plt
```

Here we plot the exponentiated coefficients, so that 
the effect is an odds ratio. If the value is larger than 1, 
the value of the covariate shown is associated with an 
increased probability of getting hypertension, and vice versa.
Moreover, a larger absolute value indicates a larger 
influence on response variable. 

We can conclude that people with diabetes, high value of 
cholesterol and fair diet have higher risk of getting 
hypertension compared to individuals with the baseline 
levels of these covariates. Also, among them, diabetes 
and cholesterol have the strongest relationship with 
hypertension, which is an accurate reflection of what 
we already know about hypertension. In all, with the 
help of a regression model like this, researchers could 
make an initial judgement about whether an individual may
have a high risk of hypertension, once these variables 
have been measured.

## Compare to logistic regression 

What would happen in this case if we were to use standard 
logistic regression using `glm()` instead of `svyglm()`

```{r}
g_svy <- glm(hypertension ~ smoking, 
          family = binomial(link = 'logit'), 
          data = hy_df)


g_svy_res <- tidy(g_svy)[-4]
g_svy_res
```


From the output, we see that all of the coefficients value 
rise. In order to compare them easily, merging these two output
into one would improve the efficiency. Only coefficient 
value and standard error are kept for better interpretation.

`merge()` function offer you an efficient way 
to merge two data frames by common columns or row names.

```{r}
merge(g_res[1:3], g_svy_res[1:3], by = 'term')
```

Therefore, compared with those who never smoke, the 
odds of getting hypertension for former smokers and current
smokers also change to `r exp(g_svy_res$estimate)[2]` 
and `r exp(g_svy_res$estimate)[3]` respectively; Second, 
standard errors calculated by svyglm are less than that 
calculated by glm, leading to a more accurate confidence 
interval; In conclusion, svyglm does better job than glm.

In the next step, let's compare the full model designed 
by these two methods. 

```{r}
g3 <- glm(hypertension ~ 
            age + bmi + cholesterol + diabetes + gender +
            smoking + diet, 
          family = binomial(link = 'logit'),
          data = hy_df)
#tidy(g3)
```

It is obvious `diabetes` and `diet` differ in exponentiated 
coefficients while that for the others vary slightly. 
Unlike the simple model, there are too many predict
variables in full model and it is cumbersome to compare 
them one by one. To guarantee a straight perception, 
we would like to visualize model output rather than show
them in a long table. 


## Visualization of model outputs

Repeat the same procedure we did for simple model,
we can construct the confidence interval manually.
Then save the information in one dataframe.

```{r}
g2_res <- tidy(g2) %>% 
  mutate(
    low = estimate - 0.95 * std.error,
    high = estimate + 0.95 * std.error,
    len = high-low,
    method = 'svyglm'
  )

g3_res <- tidy(g3) %>% 
  mutate(
    low = estimate - 0.95 * std.error,
    high = estimate + 0.95 * std.error,
    len = high-low,
    method = 'glm'
  )


res <- rbind(g2_res, g3_res)
#reorder the dataframe based on length of confidence interval
res <- res[order(res$len),][1:24,]
```

`mutate()` function is used to mutate a dataframe by 
adding new or replacing existing columns.

The label names are default setting, but to make it 
more readable, we can specify the label names.

```{r}
res$term <- c('age' ,'age' ,'bmi','bmi',
'gender','cholesterol','gender',
'cholesterol','current smoker',
'diabetic with diagnosis','current smoker',
'diabetic with diagnosis', 'former smoker',
'former smoker', 'good diet','fair diet',
'very good diet','good diet','fair diet', 
'very good diet','excellent diet',
'diabetic without diagnosis','excellent diet',
'diabetic without diagnosis') 
```

Now, let's plot the confidence interval manually.

```{r, fig.width=8, fig.height=6}
ggplot(res, aes(x = estimate, y = term),
     group_by(res2$method)) +
     geom_vline(xintercept = 0, linetype = "dashed") +
     geom_errorbarh(aes(estimate, term, xmin = low, 
     xmax = high, color = factor(method),width = 0.3), size = 0.8)+
     geom_point(aes(color = factor(method),
                    shape = factor(method)),size = 3)+
     theme(axis.title.x = element_blank(),
     axis.title.y = element_blank()) +
    ggtitle(expression(atop(bold("95% condfidence interval"))))
```

```{r, message=FALSE}
ggsave("data/Finalplot.png", plot = last_plot(), device = "png")
```

The red one is the confidence interval constructed by `svyglm()`
(`ci_svyglm`) while the black one is constructed by `glm()`(`ci_glm`)

Obviously, `cholesterol`, `diabetes`, `smoking` and `diet`
have larger differences in confidence intervals comparing 
the standard glm to the survey-weighted glm results, while 
`age`, `bmi` and `gender` have only small differences. 

Different with the result when we compare simple models,
`ci_svyglm` is larger than `ci_glm`. It is strange, 
isn't it? One possible reason behind this is `survey` computes 
the standard errors with consideration of the loss of precision 
introduced by sampling weights. Weights in `glm()` simply adjust 
the weight given to the errors in the least square estimation,
so the standard errors aren't correct in our context. 

# Summary of results

In this study, we investigated risk factors that might contribute to 
hypertension and use logistic regression to deal with binary 
response variable, while accounting for survey weights. 
The use of survey weights is the most important topic in this case 
study. Thus, we apply two approaches (with and without survey weights) 
to compare their results: one is general logistic regression and 
the other is survey weight logistic regression. 

To select the most appropriate model, in the first step, 
survey weight logistic regression (`svyglm()`) is applied 
considering our survey weight data set. Next, we delete 
several insignificant variables based on their p-value. Then,
we compare it with `glm()`. In each step we begin with a simple
model and then extend to the ideal one, a full model. This is 
well-grounded since we are able to analyze how the output changes
by setting all other variables stable. There is no doubt 
the same condition is the cornerstone of precise interpretation.

In output interpretation, we have taught you how to analyze 
the odds of getting hypertension with one categorical variable
example and one numerical variable example. Besides, we find
`diabetes` and `cholesterol` have strong positive effects on 
the probability of getting hypertension. And confidence intervals 
for `cholesterol`, `diabetes`,`smoking` and `diet` have obvious 
difference. It is wired that in simple model, `ci_svyglm` is shorter
than `ci_glm` while in full model the results change to the opposite. 
One potential explanation is the incompleteness of `glm()`. But 
`svyglm()` are more likely to give an accurate results since it 
consider sampling weight. Therefore we still believe, for survey 
weight data, `svyglm()` should be the first choice than others.

In the future, we can make more efforts on applied survey methods
and decide whether there exist better approaches, such as stratified sample. 

