---
title: OpenCaseStudies - Identifying factors that contribute to hypertension using NHANES data
author: OpenCaseStudies Team
output:
  html_document:
    code_download: yes
    keep_md: false
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE,warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, 
                      warning = FALSE)
options(digits = 2)
```

# Motivation

Hypertension is one of the most common diseases in the world. 
It has been associated with myocardial infarction, stroke, renal 
failure, and death if not detected early and treated appropriately.
Some [health-realted organizations](https://www.cdc.gov/brfss/) did relevant work, 
which shows around 75 million American adults (32%) are estimated to have 
high blood pressure, costing the US around $48.6 billion 
each year. This total includes the cost of healthcare services, 
medications to treat high blood pressure, and missed days of work.

The link between hypertension and physical measurements has 
been well-established in previous studies. In this case study, we will explore the relationship between hypertension and a variety of risk factors.

This case study introduces survey methods for logistic regression, utilizing survey-weighted logistic regression and comparing the results to standard logistic regression. The plot below summarizes the results from this analysis 
illustrating that the standard error of coefficients calculated by the 
two models ([logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) 
and [survey-weighted logistic regression](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=9591E971AF4061BBF8F98083422FF313?doi=10.1.1.151.6423&rep=rep1&type=pdf))
are different. We will discuss later which model is a better choice 
for this dataset. 

<center>
![](data/FinalPlot.png)
</center>

The libraries used in this study are listed in the following table, 
along with their purpose in this particular case study:

|Library|Purpose|
|---|--------------------------------------------------------------------------------------------------------------|
|`ggplot2`| A system for declaratively creating graphics|
|`ggpubr`|Provides some easy-to-use functions for creating and customizing 'ggplot2'- based publication ready plots|
|`ggrepel`|Provides text and label geoms for 'ggplot2' that help to avoid overlapping text labels|
|`tidyverse`|A coherent system of packages for data manipulation, exploration and visualization |
|`kableExtra`|Helps with building common complex tables and manipulating table styles; creates awesome HTML tables|
|`survey`|Provides useful functions for analyzing complex survey samples|
|`haven`|A useful tool to import and export data from SAS, STATA, and SPSS formats|
|`plotrix`|A variety of plots, various labeling, axis and color scaling functions|
|`ggstance`|Provides flipped components for 'ggplot2': horizontal versions of 'Stats' and 'Geoms', and vertical versions of 'Positions'|
|`broom`|Takes the messy output of built-in functions in R, such as lm, nls, or t.test, and turns it into tidy data frames|


In order to run the code for this case study, please ensure you have these packages installed. 

The learning objectives for this case study include:

  * data visualization
  * logistic regression
  * survey-weighted analysis
  * selection of survey weights for unbalanced data

# What is the data?

For this case study, we will use the [New York City (NYC) Health and Nutrition Examination Survey (NYC HANES)](http://nychanes.org/), 
modeled on the 
[National Health and Nutrition Examination Survey (NHANES)](https://wwwn.cdc.gov/nchs/nhanes/default.aspx). NHANES is a population-based, cross-sectional study with data collected 
from a physical examination and laboratory tests, as well as a face-to-face 
interview and an audio computer-assisted self-interview (ACASI). It is 
designed to assess the health and nutritional status of adults and children 
in the United States. NYC HANES is a local version of NHANES, which implies 
it mainly focus on New York area. 

To access the NYC HANES data, go to the [NYC HANES data page](http://nychanes.org/data/) and  click the **NYC HANES Analytics Datasets** link; it will download automatically.  (A static link to this datafile can be found [here](http://nychanes.org/wp-content/uploads/sites/6/2019/01/public_v3_122018.sas7bdat).)

The data we will be using in this case study was collected from August 2013 to June 2014 and is called the 
**NYC HANES 2013-14 Blood Pressure Data**.  The survey used a probability sample of non-institutionalized adult 
New York City residents (ages 20 years or older) to provide 
representative citywide estimates. For further details, please refer to 
the [NYC HANES website](http://nychanes.org/).

In addition to the datafile, there are other useful resources available on the website including: 

* [Data Documents](http://nychanes.org/wp-content/uploads/sites/6/2019/01/28283961_Data-Documentation.pdf): 
Provides information on how best to analyze the NYC HANES 2013-14 dataset, given its population-based 
and clustered sampling scheme.
* [Analytics Guideline](http://nychanes.org/wp-content/uploads/sites/6/2015/11/ANALYTIC-GUIDELINES-2016_V2.pdf): 
Provides overall guidance on the use of the NYC HANES 2013-14 dataset and statistical weights, 
as well as other analytic issues pertaining to assessing statistical reliability of estimates.
* [Variable Codebook](http://nychanes.org/wp-content/uploads/sites/6/2019/01/28283961_NYC-HANES_codebook_Public_V3_011019.pdf): Defines the variables included in the dataset and describes how values for these variables are coded.
* [Weight Adjustment](http://nychanes.org/wp-content/uploads/sites/6/2015/11/NYC-HANES-Training-Slides_part-2_08222016.pdf): 
Explains how NYC HANES data are weighted in order to compensate for unequal probability of selection and explains how to choose the correct weight for analysis.
* [Questionnaire](http://nychanes.org/wp-content/uploads/sites/6/2015/11/28283961_NYC-HANES-2013-14_Questionnaire.pdf): Shows the questionnaire that participants in the study completed.

All of these documents enable data analysts to understand the definitions and coding of the variables and then complete the analysis appropriately.

# Data import

First we load the libraries needed for the case study.

```{r library-load}
library(knitr)
library(ggplot2)
library(ggpubr)
library(ggrepel)
library(tidyverse)
library(kableExtra)
library(survey)
library(haven)
library(broom)
library(plotrix)
```

The NYC HANES data file we are working with is a SAS formatted file, so 
we will use the function `read_sas()` from the `haven` library 
to read in the data and create a [tibble](https://tibble.tidyverse.org) 
(or `tbl_df`) in `R`. Tibbles are nice because they do not change variable 
names or data types, and they have an enhanced `print()` method which
makes it easier to view the data when working with large datasets containing complex objects. 
The [`haven` library](https://www.rdocumentation.org/packages/haven/versions/2.1.0) 
is useful to import and export files saved in a variety of formats such as
[SAS, STATA, and SPSS](http://stanfordphd.com/Statistical_Software.html). 

We now read in the data and check the dimensions of the data object:
```{r read-data}
dat <- read_sas('./data/d.sas7bdat')
dim(dat)
```

Our data contains `r nrow(dat)` observations on `r ncol(dat)` 
different variables. For our analysis, we will only consider a subset of these variables.


# Data wrangling

## Select the variables (or columns)

This is a survey dataset based on interviews and questionnaires
with `r ncol(dat)` variables. Some variables are not relevant 
to our current research question, such as 
_'LAQ1: What language is being used to conduct this interview'_. 

Previous work has shown hypertension is associated  with 
drinking, smoking, cholesterol values, and triglyceride levels. 
Instead of looking at these same risk factors, we will consider whether other variables -- which 
at first might not seem highly related to hypertension -- have an association with 
hypertension after all. We selected 13 covariates for our 
analysis.  

We will use the `select()` function from the `dplyr` package
to choose and rename the columns that we want. 

Here is a simple example to show how the renaming of the column names works:

```{r}
rename <- 
  dat %>% 
    select(id = KEY,
           race = DMQ_14_1,
           diabetes = DX_DBTS)
colnames(rename)
```

In this example, we select the three variables of `KEY`, `DMQ_14_1`, and `DX_DBTS` and rename them to the more descriptive `id`, `race`, and `diabetes`.  We save this smaller and renamed data frame in the `rename` object rather than write over our originial data. Undoubtedly, compared with `DMQ_14_1` and `DX_DBTS`, 
`race` and `diabetes` are more readable and more easily to understand.

Now we select and rename the 13 variables we will consider in our analysis:

```{r select-cols}
hy_df <- 
  dat %>% 
    select(id = KEY,
           age = SPAGE,
           race = DMQ_14_1,
           gender = GENDER,
           diet = DBQ_1,
           income = INC20K,
           diabetes = DIQ_1,
           bmi = BMI,
           cholesterol = BPQ_16,
           drink = ALQ_1_UNIT,
           smoking = SMOKER3CAT,
           hypertension = BPQ_2,
           surveyweight = EXAM_WT)
```

We will give some description of each variable below, but for full details we refer the reader to the
[Variable Codebook](http://nychanes.org/wp-content/uploads/sites/6/2019/01/28283961_NYC-HANES_codebook_Public_V3_011019.pdf).



### Non-categorical variables 

There are four non-categorial variables that we will use in our analysis:

  * `id`: Sample case ID, unique to each individual in the sample
  * `age`: Sample age, range 22-115 years
  * `bmi`: BMI = $kg/m^2$ where $kg$ is a person's weight in kilograms and $m$ is their height in meters
  * `surveyweight`: Numeric values associated with each observation to let us know how much weight the observation should receive in our analysis (more details later)
  
### Categorical variables 

We will consider ten categorial variables: 

  * `race`: 
    + 100 = White
    + 110 = Black/African American
    + 120 = Indian
    + 140 = Native Hawaiian/Other Pacific Islander
    + 180 = Asian
    + 250 = Other race
  * `gender`:
    + 1 = Male
    + 2 = Female
  * `born`:
    + 1 = US born
    + 2 = Other country
  * `diet`: 
    + 1 = Excellent
    + 2 = Very good 
    + 3 = Good
    + 4 = Fair
    + 5 = Poor
  * `diabetes`:  Has SP ever been told by a doctor or health professional that SP has diabetes or sugar diabetes?
    + 1 = Yes
    + 2 = No 
    + 3 = Prediabetes
  * `cholesterol`: an oil-based substance. If concentrations get too high, it puts people at risk of heart diseases
    + 1 = High cholesterol value
    + 2 = Low cholesterol value
  * `drink`: In the past 12 months, how often did sample drink any type of alcoholic beverage
    + 1 = Weekly
    + 2 = Monthly
    + 3 = Yearly
  * `smoke`: 
    + 1 = Never smoker
    + 2 = Current smoker
    + 3 = Former smoker
  * `income`:
    + 1 = Less than $20,000
    + 2 = $20,000 - $39,999
    + 3 = $40,000 - $59,999
    + 4 = $60,000 - $79,999
    + 5 = $80,000 - $99,999
    + 6 = $100,000 or more
  * `hypertension`: Previously diagnosed as having hypertension
    + 1 = Yes
    + 2 = No
    
## Initial data inspection

The first step of any data analysis should be to explore 
the data through data visualizations and data summaries like
tables and summary statistics. There are several ways that you 
can have an initial glance at your data. The `summary()` or `head()` functions in are excellent 
ways to help you have a quick look at the data set.

The `summary()` function tabulates categorical variables and 
provides summary statistics for continuous variables, while also 
including a count of missing values, which can be very important 
in deciding what variables to consider in downstream analysis.

```{r}
summary(hy_df)

```
We see that now the `summary()` function gives us counts for each category of these variables rather than the meaningless numerical summaries, like means, it was giving before.

We see that certain variables have a large number of `NA` values; in 
particular `drink` has `r sum(is.na(hy_df$drink))` 
`NA` values and `diabetes` has `r sum(is.na(hy_df$diabetes))` 
`NA` values. Directly removing rows containing missing data is not desirable 
considering the large number of such rows, so we decided to look more 
closely at the missing values. Using the 
[Variable Codebook](http://nychanes.org/wp-content/uploads/sites/6/2019/01/28283961_NYC-HANES_codebook_Public_V3_011019.pdf) again, 
we found another variable `AlQ_1` (how often did the survey participant drink any type of alcoholic beverage), 
where 0 means they never drink.

Let's look at the frequency of counts of the variable with 
function `table()`.

```{r}
table(dat$ALQ_1)
```

Now we see why there are so many missing values for `drink`. 
Among these `r sum(is.na(hy_df$drink))` missing values, 
`r sum(dat$ALQ_1 == 0, na.rm = TRUE)` samples never drink and 
there are just 
`r sum(is.na(hy_df$drink)) - sum(dat$ALQ_1 == 0, na.rm = TRUE)`
actual missing values. Therefore, merging these two variables 
as one is a better way to capture drinking that includes those who never drink. We label those subjects who answered `0` to variable `AlQ_1` (those who never drink) as 4.
```{r}
hy_df$drink[which(dat$ALQ_1==0)] <- 4
```

Now our variable `drink` has values as follows:

  * `drink`: In the past 12 months, how often did sample drink any type of alcoholic beverage
    + 1 = Weekly
    + 2 = Monthly
    + 3 = Yearly
    + 4 = Never

And we can see only 6 missing values for `drink` are left now.
```{r}
summary(hy_df$drink)
```


```{r}
dim(hy_df)
```

## Adjust data types

From the data summaries above, we can see that there are several 
categorical variables like `race`, `gender`, `born`, `diet`, `income`, 
`diabetes`, `bmi`, `drink`, and `smoke`, which are currently being 
treated as numerical values, but should be treated as factors. Treating the data as numerical instead of categorial could result in problems in the downstream analysis.

For example, the linear model `lm()` and generalized linear model `glm()` 
functions will treat numerically-coded categorical variables as 
continuous variables, which will give the wrong result. Instead, 
we want to convert these categorical variables to factors. Before doing 
this, we want to get a better understanding of exactly what values 
are stored in the different variables. We refer to the 
[Variable Codebook](http://nychanes.org/wp-content/uploads/sites/6/2019/01/28283961_NYC-HANES_codebook_Public_V3_011019.pdf) 
to get the correct mapping of the numerical values 
to the category labels.

We can use the `factor()` function in base R to convert each 
variable and assign the correct levels. Any values that are not 
included in the `levels` argument will get set to `NA` values. 
We also want to think about creating a natural ordering to the 
factor levels here: the first level will generally be our 
reference level in a linear model, so it makes sense to try
to give them an order that reflects our choice of reference group.  For example, we will want to compare individuals drink weekly, monthly, or yearly to those who never drink, so we order the `drink` variable as `Never`, `Weekly`, `Monthly`, `Yearly` even though the numerical values in that order are 4, 1, 2, 3.

```{r recodeFactors}
hy_df$race <- factor(hy_df$race, 
                     levels=c(100, 110, 120, 140, 180, 250), 
                     labels=c('White', 'Black/African American', 
                              'Indian /Alaska Native', 
                              'Pacific Islander', 
                              'Asian', 'Other Race'))

hy_df$gender <-  factor(hy_df$gender, levels=c(1,2), 
                        labels=c('Male', 'Female'))

hy_df$diet <-  factor(hy_df$diet, levels=c(5:1), 
                      labels=c('Poor', 'Fair', 'Good', 
                               'Very good','Excellent'))

hy_df$income <-  factor(hy_df$income, levels=c(1:6), 
                        labels=c('Less than $20,000','$20,000 - $39,999',
                                 '$40,000 - $59,999','$60,000 - $79,999',
                                 '$80,000 - $99,999','$100,000 or more'))

hy_df$diabetes <-  factor(hy_df$diabetes, levels=c(2,1,3), 
                          labels=c('No','Yes','Prediabetes'))

hy_df$cholesterol <-  factor(hy_df$cholesterol, levels=c(2,1), 
                             labels=c('Low value','High value'))

hy_df$drink <-  factor(hy_df$drink, levels=c(4,1,2,3), 
                       labels=c('Never','Weekly', 'Monthly', 'Yearly'))

hy_df$smoking <-  factor(hy_df$smoking, levels=c(3:1), 
                         labels=c('Never smoker','Former smoker','Current smoker'))

hy_df$hypertension <-  factor(hy_df$hypertension, levels=c(2,1), 
                              labels=c('No','Yes'))
```


# Exploratory data analysis

Simple data visualizations give us a first look at 
the data and provide information about how the different 
variables are related to one another. Plots can identify the trends 
or patterns in the variables of interest and inform the next steps in the data analysis. For our 
data visualizations, we will mainly use the package `ggplot2`, 
a powerful tool for data visualization. A link for its cheat sheet is [here]( https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf).

The type of plot we will make will depend on the type of variable(s) we are plotting.  First, we try to plot one categorical variable (`hypertension`) with 
one numerical variable (`age`). Since `hypertension` is the categorical variable, we can compare the age distributions between those who are hypertense and those who are not using side-by-side boxplots.  Here the variable on the `x` axis is `hypertension` and the variable on the `y` axis is `age`, which corresponds to the `aes(x=hypertension, y=age)` definition in the `ggplot` aesthetic definition.

One thing you need to pay attention to is the missing value, which would result in a wrong plot. In the next step, we aim to remove rows containing missing 
data with the function `drop_na()` in library `tidyr` and store in a new data frame:

```{r}
hy_p_df <- 
  hy_df %>%
  drop_na()
```

This will drop all rows which still contain missing values. By checking the dimensions of our new data frame, `hy_p_df`, we see that we retain `r nrow(hy_p_df)` observations with `r ncol(hy_p_df)` different variables.


```{r, fig.width=4, fig.height=4}
p1 <- hy_p_df %>% 
        ggplot(aes(x = hypertension, y = age)) +
          geom_boxplot() + 
          ggtitle('Distribution of age by hypertension status')
p1
```

Comparing the medians of these two boxplots, we see that people who are hypertense tend to be older than people who are not, which indicates that age is related to hypertension.

What about a plot for investigating the relationship between `hypertension` and `gender`? Let's try three different ways to plot the categorical variable `gender` with `hypertension`.  We'll use the function `ggarrange()` in [package `ggpubr`](https://www.rdocumentation.org/packages/ggpubr/versions/0.2) to arrange multiple ggplots on the same page. 

```{r, fig.width=11, fig.height=5}
p2 <- hy_p_df %>% 
        ggplot(aes(x = hypertension, y=gender)) + 
          geom_boxplot() + ggtitle('distribution of gender')
p3 <- hy_p_df %>% 
        ggplot(aes(x = hypertension, fill = gender)) + 
          geom_bar() + ggtitle('distribution of gender')
p4 <- hy_p_df %>% 
        ggplot(aes(x = hypertension, fill = gender)) + 
          geom_bar(position = "fill") + 
          ggtitle('distribution of gender') + 
          ylab('proportion')
ggarrange(p2, p3, p4, ncol=3, nrow=1)
```

The left plot uses `geom_boxplot()` as we did with `age`, but 
it fails to show the relationship of interest! Boxplots are not what we want 
for a categorical variable like `gender`, so we need another plotting method. 

The middle and right plots use barplots to look at the 
relationship between `hypertension` and `gender`. This time it works! The middle plot shows the count of males and females for those with and without hypertension.  The right plot more clearly shows the proportion of males and females within each hypertension group by using the `position='fill'` option in the `geom_bar()` function. The y-axis in the right plot is proportion rather than count. 
From this visualization, we see that a higher proportion the hypertense individuals are male compared to the non-hypertense individuals.

But are these plots really the right ones for our question of interest?  Rather than comparing the distribution of gender between the hypertension groups, we really want to see whether there is a difference in hypertension rates between gender groups. To do this, we switch the x-axis and y-axis, to show `hypertension` as a 
function of `gender`, rather than the other way around. 

```{r, fig.width=11, fig.height=5}
p3b <- hy_p_df %>% 
        ggplot(aes(x = gender, fill = hypertension)) + 
          geom_bar() + ggtitle('distribution of hypertension')
p4b <- hy_p_df %>% 
        ggplot(aes(x = gender, fill = hypertension)) + 
          geom_bar(position = "fill") + 
          ggtitle('distribution of hypertension') + 
          ylab('proportion')
ggarrange(p3b, p4b, ncol=2, nrow=1)
```

Now we can compare the distribution of hypertension between males and females.  
From this visualization we see that a higher proportion of males than females in our dataset have hypertension. 

Next, we apply the same visualization methods on other variables to explore relationships between them.

```{r, fig.width=9, fig.height=5}
p5 <- hy_p_df %>% 
        ggplot(aes(x = smoking, fill = drink)) + 
          geom_bar(position = "fill") + 
          ggtitle('smoking and drink') + ylab('proportion')
p6 <- hy_p_df %>% 
        ggplot(aes(x = cholesterol, fill = diabetes)) + 
          geom_bar(position = "fill") + 
          ggtitle('cholesterol and diabetes') + ylab('proportion')
ggarrange(p5, p6, ncol=2, nrow=1)
```

From the left plot, we see that there are higher proportions of monthly and yearly drinkers in current smokers, and a higher proportion of weekly drinkers in former smokers.  This may imply a potential relationship between smoking and drinking. 
Additionally, the right plot shows that there is a much higher proportion of
diabetes, both diagnosed and undiagnosed, in the high cholesterol group compared to the low cholesterol group.  Therefore a relationship also appears to exist between these two variables. 

What if we want to explore a relationship between more than two variables? 
Suppose we wanted to look at the relationship between BMI, cholesterol level, and smoking status all at the same time.  Since `cholesterol` and `smoking` are both categorical variables, we can divide by the levels of one of these variables, say `smoking`, and then look at plots showing the relationship between the other two variables within each level of `smoking`.

We can do this in `ggplot` by taking the plot statement for two variables (`cholesterol` and `bmi`) and adding a `facet_wrap()` function for the third variable we're dividing by.

```{r}
p7 <- hy_p_df %>% 
        ggplot(aes(x = cholesterol, y = bmi, fill = cholesterol)) +
          geom_boxplot() + facet_wrap(~ smoking, ncol = 3)
p7
```

The function `facet_wrap(~ smoking, ncol = 3)` uses the 
faceting capability to produce a plot with three panels 
(one panel for each smoking category). Within each panel 
is a boxplot which characterises the distribution of `bmi` for 
each cholesterol category within that `smoking` category. We 
realize that there is a tendency of increasing `bmi` from never 
smoker to current smoker, and the highest `bmi` values are 
found in current smokers.

These data visualizations have revealed potential relationships between our variables.  The next step is to choose which variables
we might include our statistical model in the next section.  

# Data analysis

Now that we have spent some time cleaning the data and looking at 
data visualizations, we want to use a statistical 
model to address our question of interest about which 
factors are related to the risk of hypertension.

Which model should we use? Since we are looking 
at whether or not someone develops hypertension, our outcome
variable (`hypertension`) is **binary**. A binary outcome means a logistic regression model is a natural choice.  However, think of the nature of our dataset and how it was collected. It is data obtained from a survey, and we have to account for this during the analysis of the data.

In a survey sample, we often end up with "too many" samples in a category, often due to the designed sampling plan.  By "too many", we mean more than would be expected based on the make-up of the population from which we are sampling.  For example, we may have a much higher proportion of women in our sample compared to the population and a much lower proportion of men than in the population. This may happen by design if we purposely *oversample* a group that isn't well represented in the overall population.

To analyze our survey data and infer back to the population, we can use data weighting to account for the mismatch between the population and sample. If we want the data to reflect the whole population, instead of treating each data 
point equally, we weight the data so that taken together,
our sample does reflect the entire community.

To appropriately analyze our data as a survey, we will use the [package `survey`](https://cran.r-project.org/web/packages/survey/survey.pdf), which contains functions for various types of analysis that account for survey design.

## Survey weights 

### What are survey weights?

Suppose that we have 25 students (20 male and 5 female) in your biostatistics
class, and we want to talk with 5 of them to gauge their 
understanding of the content in the class. Although the proportion of female students in the population is small, we are very interested in getting their opinion, so we want to be sure to have some female students in our sample.  By randomly sampling 5 students 
from the class, it's quite possible we could end up with all male students in our sample, and we wouldn't learn anything about the female perspective in the class. 

Consider the extreme case where we are going to require that 4 of the 5 people we sample are female students, to be sure we get good information about the female perspective.  We sample 4 of the 5 female students and 1 of the 20 male students.   Do we expect this sample to represent the population? Definitely not, since there is a higher proportion of females in the sample than the population. We can 
correct for this by weighting our samples so that, taken together, 
they better reflect the composition of the population we want to learn about. 

Let's assume we sampled 4 of the 5 female students and 1 of the 20 male students from our population. To calculate the survey weights, we could use the following formula:

$$Weight = \frac{Proportion~in~population}{Proportion~in~sample}$$
$$w_m=Male~Weight = \frac{20/25}{1/25} = 20$$
$$w_f=Female~Weight = \frac{5/25}{4/25} = 1.25$$
We can interpret these weights by saying that each male student in the sample represents 20 male students in the population and each female student in the sample represents 1.25 female students in the population.  Mathematically, we can see this as:

$$ 1~observed~male* w_m = 20~males $$ 
and 
$$ 4~observed~females * w_f = 5~females$$ 

<center>
![](data/surveyweight.jpeg)
</center>

By weighting the observations, we make the sample better 
represent the population.

For complex survey sampling designs, it can be 
complicated to calculate the weight for each individual observation. However, for many large survey data sets, such as NHANES, the appropriate weight is calculated by the organization that administers the survey and provided as a variable in the dataset. In our case study, this survey weight is calculated and provided as the `surveyweight` variable and we can simply 
apply this weight and perform a **survey-weighted logistic regression**.

### Selecting the weights

Because the NYC HANES 2013-2014 data have been collected to 
address a variety of different questions and using different 
surveys, the researchers who produced the data have employed a 
somewhat complex weighting scheme to compensate for unequal 
probability of selection. Five sets of survey weights have been 
constructed to correspond to different sets of variables that were
collected: CAPI  weight, Physical weight, Blood Lab result weight,
Urine Lab results weight and Salica Lab results weight. 
**The determination of the most appropriate weight to use for a specific analysis depends upon the variables selected by the data analyst**. 

We will give a table to indicate each variable's origin stream:


| Variable names   |      Component      |
|---------------------------------|---------------------------------|
| age                                   | CAPI                                                                                                                                                                 |
| race                                  | CAPI                                                                                                                                                                 |
| gender                                | CAPI                                                                                                                                                                 |
| diet                                  | CAPI                                                                                                                                                                 |
| income                                | CAPI                                                                                                                                                                 |
| diabetes                               | CAPI                                                                                                                                                               |
| cholesterol                           | CAPI                                                                                                                                                                 |
| drink                                 | CAPI                                                                                                                                                                 |
| smoking                               | CAPI                                                                                                                                                                 |
| hypertension                           | CAPI                                                                                                                                                                |
| bmi                                    | EXAM                                                                                                                                                                |


When an analysis involves variables from different components
of the survey, the analyst should decide whether the outcome 
is inclusive or exclusive, and then choose certain weights. 
To learn how to use weights for different purposes, refer to the particular
[Analytics Guidelines](http://nychanes.org/wp-content/uploads/sites/6/2015/11/ANALYTIC-GUIDELINES-2016_V2.pdf) for the survey. 

In our case, we choose EXAM weight since our analysis is exclusive. Do 
you remember we have moved all of the missing value? Now our dataset is 
limited to those who receive physical exam test, which means all of our 
survey participants have a value for the `EXAM_WT` variable. We've select
this variable and renamed it as `surveyweight` in the earlier data 
cleaning part of this analysis. 

## Finite population correction factor

There is one more technical detail that we need to 
consider when using survey data. Many methods for analysis 
of survey data make the assumption that
**samples were collected using sampling with replacement**, 
i.e., any time a new participant is drawn, each member in the population has 
an equal chance of being sampled, even if they have 
already been sampled. This is not usually how surveys
are actually carried out, so an adjustment may be necessary
to reflect this difference. This adjustment is called the
**finite population correction factor** and it is defined as:

$$FPC = (\frac{N-n}{N-1})^{\frac{1}{2}}$$
 
* `N` = population size
* `n` = sample size

In the case when the assumption above is violated (e.g. if you 
are sampling a sufficiently large proportion of the population), 
then you might sample the same persion twice. The finite 
population correction (FPC) is used to reduce the variance when
a substantial fraction of the total population of interest has 
been sampled. We can find the value of `N` and `n` for our survey from the
[Analytics Guidelines](http://nychanes.org/wp-content/uploads/sites/6/2015/11/ANALYTIC-GUIDELINES-2016_V2.pdf). 
Next let's calculate the FPC as below:

```{r}
N <-  6825749
n <- dim(hy_df)[1]
((N-n)/(N-1))^0.5
```

The FPC of our data set is very close to 1 since our sample is quite small compared to the size of the population, and we could simply ignore the FPC. But technically, since the data were
collected through sampling without replacement, 
it is more appropriate to use it.

## Specify the survey design

We now need to figure out how to specify the survey design 
and incorporate the sampling weights in our modeling steps. 
To help us do this, we use the function `svydesign()` in 
[package `survey`](https://cran.r-project.org/web/packages/survey/survey.pdf).
This function combines a data frame and all the design information needed to specify 
a survey design. Here is the list of options provided 
in this function:

* `ids`: Formula used to specify the cluster sampling design. *Cluster sampling* is a multi-stage sampling design where the total population is divided into several clusters and a simple random sample of clusters are selected. Then a sample is taken from the elements of each selected cluster. Use `~0` or `~1` as the formula when there are no clusters.

* `data`: Data frame (or database table name) containing the variables for analysis look up variables in the formula arguments.

* `weights`: Dormula or vector specifying the sampling weights. 

* `fpc`: Finite population correction, `~rep(N,n)`  generates a vector of length n where each entry is N (the population size). Default value is 1. The use of fpc indicates a sample without replacement, otherwise the default is a sample with replacement.
 
* `strata`: Specification for stratified sampling.  *Stratified sampling* is a sampling design which divides members of the population into homogeneous subgroups and then samples independently in these subpopulations. It is advantageous when subpopulations within an overall population vary.
 
In our situation, we don't have any clusters or stratified sampling to specify, we just need to include the appropriate survey weights provided with the data.  We will not include a FPC, since our FPC was approximately 1.
 
Here's how we specify the design relative to our dataset:

```{r}
hypertension_design <- svydesign(
  id = ~1,
  #fpc = ~rep(N,n),
   weights = ~hy_df$surveyweight,
  data = hy_df
)
```
```{r}
#this part is used to compare the svy-design with and without missing value
hypertension_design2 <- svydesign(
  id = ~1,
   weights = ~hy_p_df$surveyweight,
  data = hy_p_df
)

v2 <- hypertension_design2$variables[,1]
v1 <- hypertension_design$variables

list1 <- as.numeric(row.names(v1[v1$id %in% v2,]))
plot(hypertension_design$prob[list1],hypertension_design2$prob)
```


The arguments are interpreted as the following:

* `ids = ~1` means there is no cluster sampling
* `data = hy_df` tells `svydesign` where to find the variables for analysis
* `weights= ~hy_df$surveyweight` tells it where to find the weight in our data frame

We can use `summary()` to show the results:
```{r}
summary(hypertension_design)
```

"Independent sampling design" means our sampling design is a 
simple random sample (SRS). By setting other parameters 
it is possible to specify different kinds of designs, such as 
stratified sampling, cluster sampling, or other multi-stage designs.


## Calculate survey-weighted summary statistics

Once we have created our `svydesign` object, we can use the
convenient `svy*` functions to calculate summary statistics that
account for survey design features.

To calculate the mean and its standard error, use the function 
`svymean()`. The `svymean()` function calculates a weighted estimate for the mean by weighting each observation with it's sampling weight.
We can compare this result to ignoring the survey weights using the
`mean()` and `std.error()` functions in base R.

Here we look at both the weighted and un-weighted mean BMI:

```{r}
svymean(~bmi, hypertension_design)
```
```{r}
mean(hy_df$bmi)
std.error(hy_df$bmi)
```

There is not a very large difference between 
these two values and their standard errors.  However, the survey-weighted results are "better" because they account for the sampling design of the HANES NYC survey.

To calculate a survey-weighted confidence interval for mean BMI value, we use the 
function `confint()` directly on the `svymean()` function:

```{r}
confint(svymean(~bmi, hypertension_design))
```

Statistics for subgroups are also easy to calculate with the
function `svyby()`.  Here we look at mean BMI within groups defined by diet quality.
```{r}
svyby(~bmi, by=~diet, design = hypertension_design, 
      FUN = svymean)
```

If we are particularly interested in one subgroup of individuals, 
we can use the `subset()` to define a design for our subgroup of interest.  For example, if we are only interested in learning about the female population:
```{r}
h_design_female <- subset(hypertension_design,gender=="Female")
svymean(~bmi, h_design_female)
```

We estimate that the mean BMI for females in the population is 28.1.

Note that if we are limiting our analysis to a subgroup of the data, we **must** use the subset command to define a new survey design that relates to this new subpopulation.  This is because the survey weights need to be updated to reflect how the data represents this new population.  The `subset` command will appropriately update the survey weights so the analysis reflects the survey design of the subsetted data.  We **cannot** simply use a subset of the data with the original survey design.


## Survey-weighted logistic regression

### Fit a simple model

Logistic regression is widely used to when the response variable is binary.  The standard logistic regression equation can be written as:

$logit(p)= log(\frac{p}{1-p})=X^{T}\beta$, where $p=P(Y=1)=E(Y)$ 

As we mentioned above, our data comes from a survey design so we need to take survey weights into account in our analysis.  We can do that with a survey-weighted logistic regression using the `svyglm()` function from the `survey` package.

The `svyglm()` function works similarly to using `glm()` to fit a standard logistic regression model.  The only difference is that instead of using the original data set in the `data` argument within `glm()`, we instead input the survey design object from `svydesign()` in the `design` argument in `svyglm()`.

Now we can fit our survey-weighted logistic regression model and 
look at the summarized output. We will start with a simple
model by choosing one variable, `smoking`, as a predictor:

```{r, warning=TRUE}
g <- svyglm(hypertension ~ smoking, 
    family = binomial(link = 'logit'), design = hypertension_design)
summary(g)
```

If we allow `warning = TRUE`, a warning would appear as "non-integer #successes in a binomial glm!". But everything is right here! `glm` and `svyglm` are just picky. They warn if they detect that the no. of trials or successes is non-integral, but they go ahead and fit the model anyway. If you want to suppress the warning (and you're sure it's not a problem), use `family=quasibinomial()` instead.

```{r, warning=TRUE}
g0 <- svyglm(hypertension ~ smoking, 
    family = quasibinomial(link = 'logit'), design = hypertension_design)
summary(g0)
```


The `tidy()` function in the `broom` package can provide a dataframe 
representation of the model's output. Now we can see the model output as a nice dataframe!

```{r}
g_res <- tidy(g0)
g_res
```

As in standard logistic regression using `glm()`, we see the coefficient estimate, standard error, test statistic, and p-value for each term in our model.  We can still get survey-weighted confidence intervals with the `confint()` function:
```{r}
confint(g0)
```


What does the model output tells us about the relationship 
between predictor variables and the chance of getting hypertension? 
Look at the coefficient table. For the predictor variable `smoking`, the
reference category is 'Never smoker'. So the coefficient for 'Former Smoker', `r g_res$estimate[2]`, tells us that the log odds of hypertension for a former smoker is 0.58 lower than for a never smoker.  It makes more sense to exponentiate the coefficients and interpret them as odds ratios:
```{r}
exp(g0$coefficients)
```

The exponentiated coefficient for 'Former smoker' is `r exp(g_res$estimate)[2]`, 
which means that former smokers have a 44% reduced odds of hypertension compared to never smokers. Similarly, the exponentiated coefficient for 'Current smoker' is `r exp(g_res$estimate)[3]`, meaning current smokers have a 31% reduction in the odds of hypertension compared to those who have never smoker.  What's more, we can see the 
p-values are less than 0.05 for both of these coefficients, suggesting 
these odds ratios are significantly different than 1.


### Fit a full model

Now we can fit a full model that includes all of our variables of interest: 

```{r}
g1 <- svyglm(hypertension ~ 
               age + race + gender + diet + income + 
               diabetes + bmi + cholesterol + drink + smoking,
             family = binomial(link = 'logit'), 
             design = hypertension_design)
g1_res <- tidy(g1)
g1_res %>% as.data.frame()
```

One interesting thing is that both 
the coefficients and p-values for the `smoking` variables are different than in our simple model.  Why did this happen? Remember that with other variables in the model, our interpretation of the coefficients for smoking changes.  We now have to interpret them as describing the relationship between smoking and hypertension *while holding the other variables in the model constant.*  If the other variables in the model are also related to `smoking`, then the relationship between smoking and hypertension may be difference once we account for the other variables compared to the relationship of smoking on it's own.

Now, let's interpret the new output. Again, we will interpret exponentiated coefficients as odds ratios.  One categorical
variable and one continuous variable are chosen as examples:

```{r}
exp(g1$coefficients)
```

* `smoking`: Holding all other variables constant, former smokers have 30% reduced odds of hypertension compared to those who have never smoked.  Holding all other variables constant, current smokers have 10% reduced odds of hypertension compared to those who have never smoked.

* `bmi`: Holding all other variables constant, a one-unit increase in bmi 
is associated with a 7% increase in the odds of hypertension.

### Model selection

Not all of the variables in our full model `g1` are considered statistically significant, so we would 
like to remove some of them to get a reduced model. We find 
`race`,`income`, `diet`,`drink` and `smoking` are not that 
sigificant. Contrary to our common sense, `diet`,`drink` and 
`smoking` have minimum influence on hypertension when including
the other variables in the model. We believe this is because 
that their influence may be explained by other covariates, 
such as `bmi`. Therefore, here we just remove `race`, `drink` and `income`.

```{r,fig.width=7, fig.height=5}
g2 <- svyglm(hypertension ~ 
        age + bmi + cholesterol + diabetes + gender +
        smoking + diet, 
        family = binomial(link = 'logit'), 
        design = hypertension_design)
```

Using this model output, we would like to further examine 
which covariates have the larger influence on hypertension. 
To do this, we compare their effect sizes in the plot below.

```{r}
s <- data.frame('c' = exp(g2$coefficients[2:12]),
                'n' = names(g2$coefficients)[2:12])
s <- s[order(s$c),]
l = seq(from = 2, to = 12, by = 1)

plt<- ggplot(s, aes(x = l, y = c,col = s$c)) + 
  geom_point(size = 5)  + 
  scale_colour_gradient(low ='pink', high = 'darkred')+
  geom_hline(yintercept = 1,col = 'pink',size = 1.5, linetype = 2)+
  theme_bw() + theme(plot.title = element_text(hjust = 2, vjust = -2))+
  geom_text_repel(aes(y = c,label = n), col = 'darkred')+
  ggtitle("Effect on Hypertension") + 
  ylab("Odds Ratio") + 
  xlab("Features")+
  coord_flip()
  
plt
```

Here we plot the exponentiated coefficients, or odds ratios, for each term in the model.  If the odds ratio is larger than 1, then the term is associated with an 
increased risk of hypertension.  If the odds ratio is less than 1, then the term is associated with a descreased risk of hypertention. Moreover, a larger absolute value indicates a larger 
effect on hypertension. Remember that the categorical variables are all with respect to the reference category.

We can conclude that people with diabetes (whether diagnosed or not) and those with a high level of 
cholesterol have a higher risk of 
hypertension compared to individuals with the reference 
level of these covariates. This is an accurate reflection of what 
we already know about hypertension. 

## Compare to logistic regression 

What would happen if we didn't know to use survey weighting in our analysis and instead simply used standard 
logistic regression? We can compare the results of using `glm()` to using `svyglm()` to analyze the data.

```{r}
g_glm <- glm(hypertension ~ smoking, 
          family = binomial(link = 'logit'), 
          data = hy_df)


g_glm_res <- tidy(g_glm)
g_glm_res
```


We see that the values of the smoking coefficients are different. In order to compare them to the survey results easily, we can merge these two outputs
into a single table. Only coefficients and standard errors are kept for simplicity.

The `merge()` function offer an efficient way 
to merge two data frames by common columns or row names.

```{r}
merge(g_res[1:3], g_glm_res[1:3], by = 'term')
```

The first two columns here refer to the survey-weighted model and the last two columns show the standard logistic regression results.  We see that the coefficient estimates are different between the two models and that the
standard errors calculated by the survey-weighted model are slightly wider than those calculated in the standard case.  Without survey weighting, our standard errors are smaller than they should be when taking into account the survey design.

Now let's compare the full model designed by these two methods. 

```{r}
g3 <- glm(hypertension ~ 
            age + bmi + cholesterol + diabetes + gender +
            smoking + diet, 
          family = binomial(link = 'logit'),
          data = hy_df)
```

For this larger model, we would like to compare the two methods visually rather than show them in a long table.


## Visualization of model outputs

We will repeat the same procedure we did for the simple model, construct the confidence intervals manually, and then save the information in one dataframe.

```{r}
g2_res <- tidy(g2) %>% 
  mutate(
    low = estimate - 1.96 * std.error,
    high = estimate + 1.96 * std.error,
    len = high-low,
    method = 'svyglm'
  )

g3_res <- tidy(g3) %>% 
  mutate(
    low = estimate - 1.96 * std.error,
    high = estimate + 1.96 * std.error,
    len = high-low,
    method = 'glm'
  )


res <- rbind(g2_res, g3_res)
#reorder the dataframe based on length of confidence interval
res <- res[order(res$len),][1:24,]
```

The `mutate()` function is used here to add new variables or replace existing columns in the dataframe.  The label names are a default setting, but to make it 
more readable, we can specify the label names.

```{r}
res$term <- c('age' ,'age' ,'bmi','bmi',
'gender','cholesterol','gender',
'cholesterol','current smoker',
'current smoker','diabetes',
 'former smoker','diabetes',
'former smoker', 'good diet','fair diet',
'very good diet','good diet','fair diet', 
'very good diet','excellent diet',
'prediabetes','excellent diet',
'prediabetes') 
```

Now, let's plot the confidence intervals using `ggplot`.

```{r, fig.width=8, fig.height=6}
ggplot(res, aes(x = estimate, y = term),
     group_by(res2$method)) +
     geom_vline(xintercept = 0, linetype = "dashed") +
     geom_errorbarh(aes(estimate, term, xmin = low, 
     xmax = high, color = factor(method),width = 0.3), size = 0.8)+
     geom_point(aes(color = factor(method),
                    shape = factor(method)),size = 3)+
     theme(axis.title.x = element_blank(),
     axis.title.y = element_blank()) +
    ggtitle(expression(atop(bold("95% condfidence interval"))))+
   theme_minimal()
```

```{r, message=FALSE}
ggsave("data/Finalplot.png", plot = last_plot(), device = "png")
```

In this plot, the red designates the confidence interval constructed by `svyglm()`
(`ci_svyglm`) and the blue designates the confidence interval constructed by `glm()`(`ci_glm`)

We see that `diabetes`, `smoking` and `diet`
have larger differences in confidence intervals comparing 
the standard glm to the survey-weighted glm results, while 
`age`, `bmi`, `cholesterol` and `gender` have only small differences. We also see that, in general, the confidence intervals from `ci_svyglm` are larger than those from `ci_glm`. So we would not get the "correct" results if we failed to take into account the survey weighting in our analysis.

# Summary of results

In this study, we investigated risk factors that may contribute to 
hypertension and used survey-weighted logistic regression to handle a binary 
response variable while accounting for survey weights. 
The use of survey weights is the main focus of this case 
study. Thus, we apply two approaches (with and without survey weights) 
to compare their results: one is standard logistic regression and 
the other is survey weight logistic regression. 

To select the most appropriate model, in the first step, 
survey weight logistic regression (`svyglm()`) is applied 
considering our survey weight data set. Next, we delete 
several insignificant variables based on their p-value. Then,
we compare it with `glm()`. In each step we begin with a simple
model and then extend to the ideal one, a full model. This is 
well-grounded since we are able to analyze how the output changes
by setting all other variables stable. There is no doubt 
the same condition is the cornerstone of precise interpretation.

In output interpretation, we have taught you how to analyze 
the odds of getting hypertension with one categorical variable
example and one numerical variable example. Besides, we find
`diabetes` and `cholesterol` have strong positive effects on 
the probability of getting hypertension. We show differences in both the coefficient est But 
`svyglm()` are more likely to give an accurate results since it 
consider sampling weight. Therefore we still believe, for survey 
weight data, `svyglm()` should be the first choice than others.

In the future, we can make more efforts on applied survey methods
and decide whether there exist better approaches, such as stratified sample. 

